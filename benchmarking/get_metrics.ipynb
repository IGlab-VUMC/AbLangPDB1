{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Benchmarking Pipeline with Excel Export\n",
    "\n",
    "This notebook provides a complete workflow for:\n",
    "1. Running comprehensive benchmarks across multiple models and datasets\n",
    "2. Generating standardized summary files\n",
    "3. Creating formatted Excel reports with performance rankings\n",
    "\n",
    "**Models included:**\n",
    "- AbLangPDB (cosine similarity)\n",
    "- AbLangRBD (cosine similarity)\n",
    "- AbLangPre (cosine similarity)\n",
    "- SEQID (sequence identity)\n",
    "- CDRH3ID (CDRH3 identity)\n",
    "\n",
    "**Datasets:**\n",
    "- SAbDab (structural antibody database)\n",
    "- DMS (deep mutational scanning)\n",
    "\n",
    "**Features:**\n",
    "- Automated testing to ensure Excel generation works\n",
    "- Optional embedding re-calculation toggle\n",
    "- Optional threshold reuse for faster re-runs\n",
    "- Comprehensive error handling and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  â€¢ Recalculate embeddings: False\n",
      "  â€¢ Output folder: output_csvs\n",
      "  â€¢ Excel filename: comprehensive_benchmarking_results.xlsx\n",
      "  â€¢ Batch size: 256\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION FLAGS - MODIFY THESE AS NEEDED\n",
    "# =============================================================================\n",
    "\n",
    "# Set to False to use existing parquet files (faster), True to re-calculate embeddings\n",
    "RECALCULATE_EMBEDDINGS = False\n",
    "\n",
    "# Model paths - update these paths as needed\n",
    "MODEL_PATHS = {\n",
    "    \"AbLangPDB\": \"../../../huggingface/AbLangPDB1/ablangpdb_model.safetensors\",\n",
    "    \"AbLangRBD\": \"../../../huggingface/AbLangRBD1/model.safetensors\"\n",
    "}\n",
    "\n",
    "# Batch size for embedding generation\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_FOLDER = \"output_csvs\"\n",
    "EXCEL_FILENAME = \"comprehensive_benchmarking_results.xlsx\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  â€¢ Recalculate embeddings: {RECALCULATE_EMBEDDINGS}\")\n",
    "print(f\"  â€¢ Output folder: {OUTPUT_FOLDER}\")\n",
    "print(f\"  â€¢ Excel filename: {EXCEL_FILENAME}\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import typing as T\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import local modules\n",
    "import calculate_metrics\n",
    "import calculate_metrics_dms\n",
    "import models\n",
    "from excel_generator import generate_results_excel, print_summary_stats\n",
    "from ablangpaired_model import AbLangPairedConfig, AbLangPaired\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_file_exists(filepath, description=\"\"):\n",
    "    \"\"\"Check if a file exists and return status.\"\"\"\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = \"âœ…\" if exists else \"âŒ\"\n",
    "    print(f\"  {status} {description}: {filepath}\")\n",
    "    return exists\n",
    "\n",
    "\n",
    "def embed_with_ablangpaired(input_path: str, output_path: str, model_path: str, model_name: str):\n",
    "    \"\"\"Generate embeddings using AbLangPaired models.\n",
    "    \n",
    "        Args:\n",
    "            model_name: str. If \"ablangpre\" then the model architecture will no longer have the mixer layer.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ Generating {model_name} embeddings...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_parquet(input_path)\n",
    "    if \"EMBEDDING\" in df.columns:\n",
    "        df = df.drop(columns=[\"EMBEDDING\"])\n",
    "    \n",
    "    # Setup model\n",
    "    model_config = AbLangPairedConfig(checkpoint_filename=model_path)\n",
    "    is_ablangpre = model_name == \"ablangpre\"\n",
    "    model = AbLangPaired(model_config, device=device, use_pretrained=is_ablangpre)\n",
    "    \n",
    "    # Tokenize and embed using enhanced methods\n",
    "    tokenized_dataloader = models.tokenize_data(df, model_config, batch_size=BATCH_SIZE)\n",
    "    all_embeds = models.embed_dataloader(tokenized_dataloader, model, device)\n",
    "    \n",
    "    # Save\n",
    "    df['EMBEDDING'] = list(all_embeds.cpu().numpy())\n",
    "    df.to_parquet(output_path)\n",
    "    \n",
    "    print(f\"âœ… {model_name} embeddings saved to {output_path}\")\n",
    "    return df\n",
    "\n",
    "print(\"Helper functions loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Required Base Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Checking base dataset files...\n",
      "  âœ… SAbDab base dataset: ablangpdb_renameddatasets.parquet\n",
      "  âœ… DMS base dataset: ablangrbd_renameddatasets.parquet\n",
      "  âœ… SAbDab validation labels: ablangpdb_train_val_label_mat.pt\n",
      "  âœ… SAbDab test labels: ablangpdb_train_test_label_mat.pt\n",
      "  âœ… DMS validation labels: dms_train_val_label_mat.pt\n",
      "  âœ… DMS test labels: dms_train_test_label_mat.pt\n",
      "\n",
      "âœ… All base files found!\n"
     ]
    }
   ],
   "source": [
    "# Check that base dataset files exist\n",
    "print(\"ğŸ“‹ Checking base dataset files...\")\n",
    "\n",
    "base_files = {\n",
    "    \"SAbDab base dataset\": \"ablangpdb_renameddatasets.parquet\",\n",
    "    \"DMS base dataset\": \"ablangrbd_renameddatasets.parquet\",\n",
    "    \"SAbDab validation labels\": \"ablangpdb_train_val_label_mat.pt\",\n",
    "    \"SAbDab test labels\": \"ablangpdb_train_test_label_mat.pt\",\n",
    "    \"DMS validation labels\": \"dms_train_val_label_mat.pt\",\n",
    "    \"DMS test labels\": \"dms_train_test_label_mat.pt\"\n",
    "}\n",
    "\n",
    "missing_base_files = []\n",
    "for desc, filepath in base_files.items():\n",
    "    if not check_file_exists(filepath, desc):\n",
    "        missing_base_files.append(filepath)\n",
    "\n",
    "if missing_base_files:\n",
    "    raise FileNotFoundError(f\"Missing required base files: {missing_base_files}\")\n",
    "\n",
    "print(\"\\nâœ… All base files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EMBEDDING GENERATION\n",
      "============================================================\n",
      "ğŸ“‚ Checking for existing embedding files...\n",
      "âœ… Using existing: sabdab_embeddedby_ablangrbd.parquet\n",
      "âœ… Using existing: sabdab_embeddedby_ablangpre.parquet\n",
      "âœ… Using existing: dms_embeddedby_ablangpdb.parquet\n",
      "âœ… Using existing: dms_embeddedby_ablangpre.parquet\n",
      "\n",
      "âœ… Embedding generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Define all embedding files needed\n",
    "embedding_files = {\n",
    "    # SAbDab dataset embeddings\n",
    "    \"sabdab_embeddedby_ablangrbd.parquet\": (\"ablangpdb_renameddatasets.parquet\", MODEL_PATHS[\"AbLangRBD\"], \"AbLangRBD\"),\n",
    "    \"sabdab_embeddedby_ablangpre.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AbLangPre\"),\n",
    "    \n",
    "    # DMS dataset embeddings\n",
    "    \"dms_embeddedby_ablangpdb.parquet\": (\"ablangrbd_renameddatasets.parquet\", MODEL_PATHS[\"AbLangPDB\"], \"AbLangPDB\"),\n",
    "    \"dms_embeddedby_ablangpre.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AbLangPre\")\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EMBEDDING GENERATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if RECALCULATE_EMBEDDINGS:\n",
    "    print(\"ğŸ”„ Recalculating all embeddings...\")\n",
    "    force_generate = True\n",
    "else:\n",
    "    print(\"ğŸ“‚ Checking for existing embedding files...\")\n",
    "    force_generate = False\n",
    "\n",
    "for output_file, (input_file, model_path, model_name) in embedding_files.items():\n",
    "    if force_generate or not os.path.exists(output_file):\n",
    "        print(f\"\\nğŸ”„ Generating: {output_file}\")\n",
    "        \n",
    "        try:\n",
    "            if model_name == \"AbLangPre\":\n",
    "                embed_with_ablangpaired(input_file, output_file, \"\", \"ablangpre\")\n",
    "            else:\n",
    "                embed_with_ablangpaired(input_file, output_file, model_path, model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error generating {output_file}: {str(e)}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"âœ… Using existing: {output_file}\")\n",
    "\n",
    "print(f\"\\nâœ… Embedding generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify All Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ Verifying all required files...\n",
      "  âœ… SAbDab base (AbLangPDB embeddings): ablangpdb_renameddatasets.parquet\n",
      "  âœ… DMS base: ablangrbd_renameddatasets.parquet\n",
      "  âœ… SAbDab + AbLangRBD: sabdab_embeddedby_ablangrbd.parquet\n",
      "  âœ… SAbDab + AbLangPre: sabdab_embeddedby_ablangpre.parquet\n",
      "  âœ… DMS + AbLangPDB: dms_embeddedby_ablangpdb.parquet\n",
      "  âœ… DMS + AbLangPre: dms_embeddedby_ablangpre.parquet\n",
      "  âœ… SAbDab validation labels: ablangpdb_train_val_label_mat.pt\n",
      "  âœ… SAbDab test labels: ablangpdb_train_test_label_mat.pt\n",
      "  âœ… DMS validation labels: dms_train_val_label_mat.pt\n",
      "  âœ… DMS test labels: dms_train_test_label_mat.pt\n",
      "\n",
      "âœ… All required files are available!\n"
     ]
    }
   ],
   "source": [
    "# Check that all required files now exist\n",
    "print(\"\\nğŸ“‹ Verifying all required files...\")\n",
    "\n",
    "all_required_files = {\n",
    "    # Base datasets\n",
    "    \"SAbDab base (AbLangPDB embeddings)\": \"ablangpdb_renameddatasets.parquet\",\n",
    "    \"DMS base\": \"ablangrbd_renameddatasets.parquet\",\n",
    "    \n",
    "    # Generated embedding files\n",
    "    \"SAbDab + AbLangRBD\": \"sabdab_embeddedby_ablangrbd.parquet\",\n",
    "    \"SAbDab + AbLangPre\": \"sabdab_embeddedby_ablangpre.parquet\",\n",
    "    \"DMS + AbLangPDB\": \"dms_embeddedby_ablangpdb.parquet\",\n",
    "    \"DMS + AbLangPre\": \"dms_embeddedby_ablangpre.parquet\",\n",
    "    \n",
    "    # Label matrices\n",
    "    \"SAbDab validation labels\": \"ablangpdb_train_val_label_mat.pt\",\n",
    "    \"SAbDab test labels\": \"ablangpdb_train_test_label_mat.pt\",\n",
    "    \"DMS validation labels\": \"dms_train_val_label_mat.pt\",\n",
    "    \"DMS test labels\": \"dms_train_test_label_mat.pt\"\n",
    "}\n",
    "\n",
    "missing_files = []\n",
    "for desc, filepath in all_required_files.items():\n",
    "    if not check_file_exists(filepath, desc):\n",
    "        missing_files.append(filepath)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâš ï¸ Warning: {len(missing_files)} files are missing:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"  - {file}\")\n",
    "    print(\"\\nProceeding with available files only.\")\n",
    "else:\n",
    "    print(\"\\nâœ… All required files are available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 10 model/dataset/metric combinations:\n",
      "  â€¢ ablangpdb_sabdab_cosine: AbLangPDB on sabdab using cosine\n",
      "  â€¢ ablangrbd_sabdab_cosine: AbLangRBD on sabdab using cosine\n",
      "  â€¢ ablangpre_sabdab_cosine: AbLangPre on sabdab using cosine\n",
      "  â€¢ seqid_sabdab: SEQID on sabdab using seq_identity\n",
      "  â€¢ cdrh3id_sabdab: CDRH3ID on sabdab using cdrh3_identity\n",
      "  â€¢ ablangpdb_dms_cosine: AbLangPDB on dms using cosine\n",
      "  â€¢ ablangrbd_dms_cosine: AbLangRBD on dms using cosine\n",
      "  â€¢ ablangpre_dms_cosine: AbLangPre on dms using cosine\n",
      "  â€¢ seqid_dms: SEQID on dms using seq_identity\n",
      "  â€¢ cdrh3id_dms: CDRH3ID on dms using cdrh3_identity\n"
     ]
    }
   ],
   "source": [
    "# Complete configuration for all model/dataset/metric combinations\n",
    "CONFIGS = {\n",
    "    # SAbDab Dataset Configurations\n",
    "    \"ablangpdb_sabdab_cosine\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPDB\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablangrbd_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablangrbd.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangRBD\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablangpre_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablangpre.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPre\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"seqid_sabdab\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"SEQID\",\n",
    "        \"score_type\": \"seq_identity\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"cdrh3id_sabdab\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"CDRH3ID\",\n",
    "        \"score_type\": \"cdrh3_identity\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \n",
    "    # DMS Dataset Configurations\n",
    "    \"ablangpdb_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablangpdb.parquet\",\n",
    "        \"labels_val\": \"dms_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPDB\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablangrbd_dms_cosine\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangRBD\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablangpre_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablangpre.parquet\",\n",
    "        \"labels_val\": \"dms_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPre\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"seqid_dms\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"SEQID\",\n",
    "        \"score_type\": \"seq_identity\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"cdrh3id_dms\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_train_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_train_test_label_mat.pt\",\n",
    "        \"model_name\": \"CDRH3ID\",\n",
    "        \"score_type\": \"cdrh3_identity\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(CONFIGS)} model/dataset/metric combinations:\")\n",
    "for name, config in CONFIGS.items():\n",
    "    print(f\"  â€¢ {name}: {config['model_name']} on {config['dataset_type']} using {config['score_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Available Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ablangpdb_sabdab_cosine: Ready to run\n",
      "âœ… ablangrbd_sabdab_cosine: Ready to run\n",
      "âœ… ablangpre_sabdab_cosine: Ready to run\n",
      "âœ… seqid_sabdab: Ready to run\n",
      "âœ… cdrh3id_sabdab: Ready to run\n",
      "âœ… ablangpdb_dms_cosine: Ready to run\n",
      "âœ… ablangrbd_dms_cosine: Ready to run\n",
      "âœ… ablangpre_dms_cosine: Ready to run\n",
      "âœ… seqid_dms: Ready to run\n",
      "âœ… cdrh3id_dms: Ready to run\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "  â€¢ Available configurations: 10/10\n",
      "  â€¢ Missing configurations: 0\n"
     ]
    }
   ],
   "source": [
    "# Check which configurations can actually run based on available files\n",
    "available_configs = {}\n",
    "missing_configs = []\n",
    "\n",
    "for config_name, config in CONFIGS.items():\n",
    "    files_to_check = [config[\"df_path\"], config[\"labels_val\"], config[\"labels_test\"]]\n",
    "    missing_files = [f for f in files_to_check if not os.path.exists(f)]\n",
    "    \n",
    "    if not missing_files:\n",
    "        available_configs[config_name] = config\n",
    "        print(f\"âœ… {config_name}: Ready to run\")\n",
    "    else:\n",
    "        missing_configs.append(config_name)\n",
    "        print(f\"âŒ {config_name}: Missing files - {missing_files}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "print(f\"  â€¢ Available configurations: {len(available_configs)}/{len(CONFIGS)}\")\n",
    "print(f\"  â€¢ Missing configurations: {len(missing_configs)}\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"\\nâš ï¸ Configurations that will be skipped: {', '.join(missing_configs)}\")\n",
    "\n",
    "if not available_configs:\n",
    "    raise RuntimeError(\"âŒ No configurations are available to run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Pre-computed Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Using precomputed thresholds for 8 configurations\n",
      "  â€¢ ablangpdb_sabdab_cosine: {'epitope_threshold': 0.5037, 'antigen_threshold': 0.2697}\n",
      "  â€¢ ablangrbd_sabdab_cosine: {'epitope_threshold': 0.7912, 'antigen_threshold': -0.2969}\n",
      "  â€¢ seqid_sabdab: {'epitope_threshold': 0.6684, 'antigen_threshold': 0.338}\n",
      "  â€¢ cdrh3id_sabdab: {'epitope_threshold': 0.2727, 'antigen_threshold': 0.0}\n",
      "  â€¢ ablangpdb_dms_cosine: {'epitope_threshold': -0.0419}\n",
      "  â€¢ ablangrbd_dms_cosine: {'epitope_threshold': 0.8493}\n",
      "  â€¢ seqid_dms: {'epitope_threshold': 0.6497}\n",
      "  â€¢ cdrh3id_dms: {'epitope_threshold': 0.1905}\n"
     ]
    }
   ],
   "source": [
    "# Optional: Pre-computed thresholds to skip threshold optimization\n",
    "# Uncomment and set values to reuse previous results for faster execution\n",
    "\n",
    "PRECOMPUTED_THRESHOLDS = {\n",
    "    # Comment out as desired to recalculate\n",
    "    \"ablangpdb_sabdab_cosine\": {\n",
    "        \"epitope_threshold\": 0.5037,\n",
    "        \"antigen_threshold\": 0.2697\n",
    "    },\n",
    "    \"ablangrbd_sabdab_cosine\": {\n",
    "        \"epitope_threshold\": 0.7912,\n",
    "        \"antigen_threshold\": -0.2969\n",
    "    },\n",
    "    \"ablangpre_sabdab_cosine\": {\n",
    "        \"epitope_threshold\": 0.6941,\n",
    "        \"antigen_threshold\": 0.5851\n",
    "    },\n",
    "    \"seqid_sabdab\": {\n",
    "        \"epitope_threshold\": 0.6684,\n",
    "        \"antigen_threshold\": 0.3380\n",
    "    },\n",
    "    \"cdrh3id_sabdab\": {\n",
    "        \"epitope_threshold\": 0.2727,\n",
    "        \"antigen_threshold\": 0.0000\n",
    "    },\n",
    "    \"ablangpdb_dms_cosine\": {\n",
    "        \"epitope_threshold\": -0.0419\n",
    "    },\n",
    "    \"ablangrbd_dms_cosine\": {\n",
    "        \"epitope_threshold\": 0.8493\n",
    "    },\n",
    "    \"ablangpre_dms_cosine\": {\n",
    "        \"epitope_threshold\": 0.6608\n",
    "    },\n",
    "    \"seqid_dms\": {\n",
    "        \"epitope_threshold\": 0.6497\n",
    "    },\n",
    "    \"cdrh3id_dms\": {\n",
    "        \"epitope_threshold\": 0.1905\n",
    "    }\n",
    "}\n",
    "\n",
    "use_precomputed = len(PRECOMPUTED_THRESHOLDS) > 0\n",
    "if use_precomputed:\n",
    "    print(f\"ğŸ”„ Using precomputed thresholds for {len(PRECOMPUTED_THRESHOLDS)} configurations\")\n",
    "    for config_name, thresholds in PRECOMPUTED_THRESHOLDS.items():\n",
    "        print(f\"  â€¢ {config_name}: {thresholds}\")\n",
    "else:\n",
    "    print(\"ğŸ†• Computing fresh thresholds for all configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comprehensive Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE BENCHMARK EXECUTION\n",
      "======================================================================\n",
      "Total configurations to run: 10\n",
      "\n",
      "======================================================================\n",
      "[1/10] Running: ablangpdb_sabdab_cosine\n",
      "Model: AbLangPDB, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "Using provided Epitope threshold: 0.5037\n",
      "Using provided Antigen threshold: 0.2697\n",
      "Preparing data with score_type: 'cosine' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.8090, Average Precision: 0.5419, F1 Score: 0.5567\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.7887, Average Precision: 0.5084, F1 Score: 0.5044\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/AbLangPDB_sabdab_ep_summarymetrics.txt and output_csvs/AbLangPDB_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [1/10] ablangpdb_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[2/10] Running: ablangrbd_sabdab_cosine\n",
      "Model: AbLangRBD, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "Using provided Epitope threshold: 0.7912\n",
      "Using provided Antigen threshold: -0.2969\n",
      "Preparing data with score_type: 'cosine' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.5753, Average Precision: 0.1601, F1 Score: 0.1447\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5090, Average Precision: 0.1537, F1 Score: 0.1853\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/AbLangRBD_sabdab_ep_summarymetrics.txt and output_csvs/AbLangRBD_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [2/10] ablangrbd_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[3/10] Running: ablangpre_sabdab_cosine\n",
      "Model: AbLangPre, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for TRAIN vs VAL...\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.6941\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.5851\n",
      "Preparing data with score_type: 'cosine' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6318, Average Precision: 0.0769, F1 Score: 0.1290\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5943, Average Precision: 0.1505, F1 Score: 0.2097\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/AbLangPre_sabdab_ep_summarymetrics.txt and output_csvs/AbLangPre_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [3/10] ablangpre_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[4/10] Running: seqid_sabdab\n",
      "Model: SEQID, Dataset: sabdab, Score: seq_identity\n",
      "======================================================================\n",
      "Using provided Epitope threshold: 0.6684\n",
      "Using provided Antigen threshold: 0.3380\n",
      "Preparing data with score_type: 'seq_identity' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6104, Average Precision: 0.0940, F1 Score: 0.1190\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5448, Average Precision: 0.1400, F1 Score: 0.1847\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/SEQID_sabdab_ep_summarymetrics.txt and output_csvs/SEQID_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [4/10] seqid_sabdab completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[5/10] Running: cdrh3id_sabdab\n",
      "Model: CDRH3ID, Dataset: sabdab, Score: cdrh3_identity\n",
      "======================================================================\n",
      "Using provided Epitope threshold: 0.2727\n",
      "Using provided Antigen threshold: 0.0000\n",
      "Preparing data with score_type: 'cdrh3_identity' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.5645, Average Precision: 0.0742, F1 Score: 0.0801\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5274, Average Precision: 0.1237, F1 Score: 0.1848\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/CDRH3ID_sabdab_ep_summarymetrics.txt and output_csvs/CDRH3ID_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [5/10] cdrh3id_sabdab completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[6/10] Running: ablangpdb_dms_cosine\n",
      "Model: AbLangPDB, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "\n",
      "--- Using provided F1 threshold: -0.0419 ---\n",
      "Preparing data with score_type: 'cosine' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5379, Average Precision: 0.1469, F1 Score: 0.2041\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/AbLangPDB_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [6/10] ablangpdb_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[7/10] Running: ablangrbd_dms_cosine\n",
      "Model: AbLangRBD, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "\n",
      "--- Using provided F1 threshold: 0.8493 ---\n",
      "Preparing data with score_type: 'cosine' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.8442, Average Precision: 0.6401, F1 Score: 0.5926\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/AbLangRBD_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [7/10] ablangrbd_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[8/10] Running: ablangpre_dms_cosine\n",
      "Model: AbLangPre, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for TRAIN vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6608\n",
      "Preparing data with score_type: 'cosine' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5748, Average Precision: 0.1756, F1 Score: 0.2145\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/AbLangPre_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [8/10] ablangpre_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[9/10] Running: seqid_dms\n",
      "Model: SEQID, Dataset: dms, Score: seq_identity\n",
      "======================================================================\n",
      "\n",
      "--- Using provided F1 threshold: 0.6497 ---\n",
      "Preparing data with score_type: 'seq_identity' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5579, Average Precision: 0.1904, F1 Score: 0.2083\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/SEQID_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [9/10] seqid_dms completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[10/10] Running: cdrh3id_dms\n",
      "Model: CDRH3ID, Dataset: dms, Score: cdrh3_identity\n",
      "======================================================================\n",
      "\n",
      "--- Using provided F1 threshold: 0.1905 ---\n",
      "Preparing data with score_type: 'cdrh3_identity' for TRAIN vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5315, Average Precision: 0.1369, F1 Score: 0.2023\n",
      "\n",
      "Results saved to output_csvs\n",
      "Summary metrics saved to output_csvs/CDRH3ID_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [10/10] cdrh3id_dms completed successfully!\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE BENCHMARK EXECUTION SUMMARY\n",
      "======================================================================\n",
      "ablangpdb_sabdab_cosine        âœ… Success\n",
      "ablangrbd_sabdab_cosine        âœ… Success\n",
      "ablangpre_sabdab_cosine        âœ… Success\n",
      "seqid_sabdab                   âœ… Success\n",
      "cdrh3id_sabdab                 âœ… Success\n",
      "ablangpdb_dms_cosine           âœ… Success\n",
      "ablangrbd_dms_cosine           âœ… Success\n",
      "ablangpre_dms_cosine           âœ… Success\n",
      "seqid_dms                      âœ… Success\n",
      "cdrh3id_dms                    âœ… Success\n",
      "\n",
      "ğŸ“Š Results:\n",
      "  â€¢ Successful: 10/10\n",
      "  â€¢ Failed: 0\n",
      "\n",
      "ğŸ‰ All available configurations completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Execute all available configurations\n",
    "execution_results = {}\n",
    "failed_configs = []\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"COMPREHENSIVE BENCHMARK EXECUTION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total configurations to run: {len(available_configs)}\")\n",
    "\n",
    "for i, (config_name, config) in enumerate(available_configs.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i}/{len(available_configs)}] Running: {config_name}\")\n",
    "    print(f\"Model: {config['model_name']}, Dataset: {config['dataset_type']}, Score: {config['score_type']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare arguments\n",
    "        args = {\n",
    "            \"df_path\": config[\"df_path\"],\n",
    "            \"labels_file_val\": config[\"labels_val\"],\n",
    "            \"labels_file_test\": config[\"labels_test\"],\n",
    "            \"score_type\": config[\"score_type\"],\n",
    "            \"model_name\": config[\"model_name\"],\n",
    "            \"output_folder\": OUTPUT_FOLDER\n",
    "        }\n",
    "        \n",
    "        # Add precomputed thresholds if available\n",
    "        if config_name in PRECOMPUTED_THRESHOLDS:\n",
    "            thresholds = PRECOMPUTED_THRESHOLDS[config_name]\n",
    "            if config[\"dataset_type\"] == \"sabdab\":\n",
    "                if \"epitope_threshold\" in thresholds:\n",
    "                    args[\"epitope_threshold\"] = thresholds[\"epitope_threshold\"]\n",
    "                if \"antigen_threshold\" in thresholds:\n",
    "                    args[\"antigen_threshold\"] = thresholds[\"antigen_threshold\"]\n",
    "            elif config[\"dataset_type\"] == \"dms\":\n",
    "                if \"epitope_threshold\" in thresholds:\n",
    "                    args[\"epitope_threshold\"] = thresholds[\"epitope_threshold\"]\n",
    "        \n",
    "        # Execute the benchmark\n",
    "        config[\"function\"](**args)\n",
    "        \n",
    "        execution_results[config_name] = \"âœ… Success\"\n",
    "        print(f\"\\nâœ… [{i}/{len(available_configs)}] {config_name} completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"âŒ Error: {str(e)}\"\n",
    "        execution_results[config_name] = error_msg\n",
    "        failed_configs.append(config_name)\n",
    "        print(f\"\\nâŒ [{i}/{len(available_configs)}] {config_name} failed: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPREHENSIVE BENCHMARK EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for config_name, result in execution_results.items():\n",
    "    print(f\"{config_name:30} {result}\")\n",
    "\n",
    "successful_configs = len(available_configs) - len(failed_configs)\n",
    "print(f\"\\nğŸ“Š Results:\")\n",
    "print(f\"  â€¢ Successful: {successful_configs}/{len(available_configs)}\")\n",
    "print(f\"  â€¢ Failed: {len(failed_configs)}\")\n",
    "\n",
    "if failed_configs:\n",
    "    print(f\"\\nâš ï¸ Failed configurations: {', '.join(failed_configs)}\")\n",
    "else:\n",
    "    print(\"\\nğŸ‰ All available configurations completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Excel Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Generating summary statistics...\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Total summary files found: 15\n",
      "Unique models: 5 (AbLangPDB, AbLangPre, AbLangRBD, CDRH3ID, SEQID)\n",
      "Unique datasets: 3 (dms, sabdab_ag, sabdab_ep)\n",
      "Unique score types: 3 (cdrh3_identity, cosine, seq_identity)\n",
      "Best ROC_AUC: AbLangRBD on dms (0.8442)\n",
      "Best Average_Precision: AbLangRBD on dms (0.6401)\n",
      "Best F1_Score: AbLangRBD on dms (0.5926)\n",
      "\n",
      "======================================================================\n",
      "GENERATING COMPREHENSIVE EXCEL REPORT\n",
      "======================================================================\n",
      "Collecting summary metrics from output_csvs...\n",
      "Found 15 summary files\n",
      "Creating pivot table...\n",
      "Pivot table created with 5 models and 9 metric columns\n",
      "Ranking values for formatting...\n",
      "Exporting to Excel: output_csvs/comprehensive_benchmarking_results.xlsx\n",
      "âœ… Excel file generated successfully: output_csvs/comprehensive_benchmarking_results.xlsx\n",
      "\n",
      "ğŸ‰ Comprehensive Excel report generated successfully!\n",
      "ğŸ“ File location: output_csvs/comprehensive_benchmarking_results.xlsx\n",
      "ğŸ“ File size: 5,435 bytes\n",
      "\n",
      "ğŸ“– Excel Report Contents:\n",
      "  â€¢ Models as rows (AbLangPDB, AbLangRBD, AbLangPre, SEQID, CDRH3ID)\n",
      "  â€¢ Datasets grouped as column headers (SAbDab, DMS)\n",
      "  â€¢ Metrics: ROC-AUC, Average Precision, F1 Score\n",
      "  â€¢ Best performance: Bold formatting\n",
      "  â€¢ Second best: Italic formatting\n",
      "  â€¢ Values rounded to 4 decimal places\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics\n",
    "print(\"\\nğŸ“Š Generating summary statistics...\")\n",
    "print_summary_stats(OUTPUT_FOLDER)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING COMPREHENSIVE EXCEL REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Generate the Excel file\n",
    "    excel_path = generate_results_excel(\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        excel_filename=EXCEL_FILENAME\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Comprehensive Excel report generated successfully!\")\n",
    "    print(f\"ğŸ“ File location: {excel_path}\")\n",
    "    print(f\"ğŸ“ File size: {os.path.getsize(excel_path):,} bytes\")\n",
    "    \n",
    "    # Provide usage instructions\n",
    "    print(f\"\\nğŸ“– Excel Report Contents:\")\n",
    "    print(f\"  â€¢ Models as rows (AbLangPDB, AbLangRBD, AbLangPre, SEQID, CDRH3ID)\")\n",
    "    print(f\"  â€¢ Datasets grouped as column headers (SAbDab, DMS)\")\n",
    "    print(f\"  â€¢ Metrics: ROC-AUC, Average Precision, F1 Score\")\n",
    "    print(f\"  â€¢ Best performance: Bold formatting\")\n",
    "    print(f\"  â€¢ Second best: Italic formatting\")\n",
    "    print(f\"  â€¢ Values rounded to 4 decimal places\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error generating Excel report: {str(e)}\")\n",
    "    print(\"\\nDebugging information:\")\n",
    "    print(f\"  â€¢ Output folder: {OUTPUT_FOLDER}\")\n",
    "    print(f\"  â€¢ Files in folder: {len(os.listdir(OUTPUT_FOLDER))}\")\n",
    "    \n",
    "    # List summary files found\n",
    "    import glob\n",
    "    summary_files = glob.glob(os.path.join(OUTPUT_FOLDER, \"*summarymetrics.txt\"))\n",
    "    print(f\"  â€¢ Summary files found: {len(summary_files)}\")\n",
    "    for f in summary_files[:5]:  # Show first 5\n",
    "        print(f\"    - {os.path.basename(f)}\")\n",
    "    if len(summary_files) > 5:\n",
    "        print(f\"    - ... and {len(summary_files)-5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE PIPELINE COMPLETION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ”§ Configuration:\n",
      "  â€¢ Recalculated embeddings: False\n",
      "  â€¢ Used precomputed thresholds: True\n",
      "  â€¢ Batch size: 256\n",
      "\n",
      "ğŸ“ˆ Benchmarking Results:\n",
      "  â€¢ Total configurations possible: 10\n",
      "  â€¢ Configurations attempted: 10\n",
      "  â€¢ Successful runs: 10\n",
      "  â€¢ Failed runs: 0\n",
      "\n",
      "ğŸ“Š Excel Report:\n",
      "  â€¢ Status: âœ… Generated successfully\n",
      "  â€¢ Location: output_csvs/comprehensive_benchmarking_results.xlsx\n",
      "  â€¢ Ready for analysis and sharing\n",
      "\n",
      "ğŸ”¬ Models Benchmarked:\n",
      "  â€¢ AbLangPDB (cosine)\n",
      "  â€¢ AbLangPre (cosine)\n",
      "  â€¢ AbLangRBD (cosine)\n",
      "  â€¢ CDRH3ID (cdrh3_identity)\n",
      "  â€¢ SEQID (seq_identity)\n",
      "\n",
      "ğŸ“Š Datasets Analyzed:\n",
      "  â€¢ DMS\n",
      "  â€¢ SABDAB\n",
      "\n",
      "ğŸ¯ Next Steps:\n",
      "  1. ğŸ“Š Open the Excel report for comprehensive performance comparison\n",
      "  2. ğŸ” Identify best-performing models for each dataset\n",
      "  3. ğŸ“ˆ Analyze performance patterns across different similarity metrics\n",
      "  4. ğŸ“‹ Share results with your research team\n",
      "  5. ğŸ“ Consider additional analyses based on findings\n",
      "\n",
      "ğŸ Comprehensive benchmarking pipeline completed!\n",
      "\n",
      "ğŸ“„ Report: output_csvs/comprehensive_benchmarking_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE PIPELINE COMPLETION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ”§ Configuration:\")\n",
    "print(f\"  â€¢ Recalculated embeddings: {RECALCULATE_EMBEDDINGS}\")\n",
    "print(f\"  â€¢ Used precomputed thresholds: {use_precomputed}\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Benchmarking Results:\")\n",
    "print(f\"  â€¢ Total configurations possible: {len(CONFIGS)}\")\n",
    "print(f\"  â€¢ Configurations attempted: {len(available_configs)}\")\n",
    "print(f\"  â€¢ Successful runs: {successful_configs}\")\n",
    "print(f\"  â€¢ Failed runs: {len(failed_configs)}\")\n",
    "\n",
    "if os.path.exists(os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)):\n",
    "    print(f\"\\nğŸ“Š Excel Report:\")\n",
    "    print(f\"  â€¢ Status: âœ… Generated successfully\")\n",
    "    print(f\"  â€¢ Location: {os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)}\")\n",
    "    print(f\"  â€¢ Ready for analysis and sharing\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š Excel Report:\")\n",
    "    print(f\"  â€¢ Status: âŒ Generation failed\")\n",
    "    print(f\"  â€¢ Check error messages above\")\n",
    "\n",
    "print(f\"\\nğŸ”¬ Models Benchmarked:\")\n",
    "models_run = set()\n",
    "for config_name, result in execution_results.items():\n",
    "    if \"Success\" in result:\n",
    "        config = available_configs[config_name]\n",
    "        models_run.add(f\"{config['model_name']} ({config['score_type']})\")\n",
    "        \n",
    "for model in sorted(models_run):\n",
    "    print(f\"  â€¢ {model}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Datasets Analyzed:\")\n",
    "datasets_run = set()\n",
    "for config_name, result in execution_results.items():\n",
    "    if \"Success\" in result:\n",
    "        config = available_configs[config_name]\n",
    "        datasets_run.add(config['dataset_type'].upper())\n",
    "        \n",
    "for dataset in sorted(datasets_run):\n",
    "    print(f\"  â€¢ {dataset}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Next Steps:\")\n",
    "print(f\"  1. ğŸ“Š Open the Excel report for comprehensive performance comparison\")\n",
    "print(f\"  2. ğŸ” Identify best-performing models for each dataset\")\n",
    "print(f\"  3. ğŸ“ˆ Analyze performance patterns across different similarity metrics\")\n",
    "print(f\"  4. ğŸ“‹ Share results with your research team\")\n",
    "print(f\"  5. ğŸ“ Consider additional analyses based on findings\")\n",
    "\n",
    "if failed_configs:\n",
    "    print(f\"\\nâš ï¸ Failed Configurations to Investigate:\")\n",
    "    for config in failed_configs:\n",
    "        print(f\"  â€¢ {config}: {execution_results[config]}\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"\\nâ“ Configurations Not Attempted (Missing Files):\")\n",
    "    for config in missing_configs:\n",
    "        print(f\"  â€¢ {config}\")\n",
    "\n",
    "print(f\"\\nğŸ Comprehensive benchmarking pipeline completed!\")\n",
    "print(f\"\\nğŸ“„ Report: {os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pub_clone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

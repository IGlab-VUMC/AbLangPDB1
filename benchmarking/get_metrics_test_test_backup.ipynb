{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test vs Test Benchmarking Pipeline with Excel Export\n",
    "\n",
    "This notebook provides a complete workflow for:\n",
    "1. Running comprehensive benchmarks across multiple models and datasets using **TEST vs TEST** comparisons\n",
    "2. Using **VAL vs VAL** comparisons for F1 threshold optimization\n",
    "3. Generating standardized summary files\n",
    "4. Creating formatted Excel reports with performance rankings\n",
    "\n",
    "**Models included:**\n",
    "- **AbLang Family**: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy (cosine similarity)\n",
    "- **Other Protein LMs**: AntiBERTy, BALM, ESM-2, IgBERT, Parapred (cosine similarity)\n",
    "- **Sequence-based**: SEQID (sequence identity), CDRH3ID (CDRH3 identity)\n",
    "- **Note**: ABodyBuilder2 DTW configurations are excluded from this analysis\n",
    "\n",
    "**Datasets:**\n",
    "- SAbDab (structural antibody database)\n",
    "- DMS (deep mutational scanning)\n",
    "\n",
    "**Key Differences from Train vs Test Analysis:**\n",
    "- Compares test antibodies against other test antibodies\n",
    "- Uses validation set for threshold optimization instead of training set\n",
    "- Uses test vs test label matrices and val vs val label matrices\n",
    "- Excludes ABodyBuilder2 structural similarity calculations\n",
    "\n",
    "**Features:**\n",
    "- Works with ALL available parquet files in the benchmarking directory\n",
    "- Optional embedding re-calculation toggle (disabled by default)\n",
    "- Optional threshold reuse for faster re-runs\n",
    "- Comprehensive error handling and logging\n",
    "- Supports 12 different models across both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  â€¢ Recalculate embeddings: False\n",
      "  â€¢ Recalculate summary metrics: False\n",
      "  â€¢ Output folder: output_csvs_test\n",
      "  â€¢ Excel filename: comprehensive_benchmarking_results_test.xlsx\n",
      "  â€¢ Batch size: 256\n",
      "\n",
      "ðŸ“ Note: This notebook supports test vs test comparisons:\n",
      "  â€¢ Models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy,\n",
      "           AntiBERTy, BALM, ESM-2, IgBERT, Parapred, SEQID, CDRH3ID\n",
      "  â€¢ Datasets: SAbDab and DMS\n",
      "  â€¢ Comparison type: TEST vs TEST (thresholded using VAL vs VAL)\n",
      "  â€¢ ABodyBuilder2 DTW configurations excluded\n",
      "  â€¢ Only existing parquet files will be processed\n",
      "  â€¢ Summary metrics will be reused if available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION FLAGS - MODIFY THESE AS NEEDED\n",
    "# =============================================================================\n",
    "\n",
    "# Set to False to use existing parquet files (faster), True to re-calculate embeddings\n",
    "# NOTE: This notebook now works with ALL available parquet files in the directory\n",
    "RECALCULATE_EMBEDDINGS = False\n",
    "\n",
    "# Set to False to skip benchmark recalculation if summary files exist (faster), True to always recalculate\n",
    "RECALCULATE_SUMMARYMETRICS = False\n",
    "\n",
    "# Model paths - update these paths as needed\n",
    "MODEL_PATHS = {\n",
    "    \"AbLangPDB\": \"../../../huggingface/AbLangPDB1/ablangpdb_model.safetensors\",\n",
    "    \"AbLangRBD\": \"../../../huggingface/AbLangRBD1/model.safetensors\"\n",
    "}\n",
    "\n",
    "# Batch size for embedding generation\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_FOLDER = \"output_csvs_test\"\n",
    "EXCEL_FILENAME = \"comprehensive_benchmarking_results_test.xlsx\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  â€¢ Recalculate embeddings: {RECALCULATE_EMBEDDINGS}\")\n",
    "print(f\"  â€¢ Recalculate summary metrics: {RECALCULATE_SUMMARYMETRICS}\")\n",
    "print(f\"  â€¢ Output folder: {OUTPUT_FOLDER}\")\n",
    "print(f\"  â€¢ Excel filename: {EXCEL_FILENAME}\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nðŸ“ Note: This notebook supports test vs test comparisons:\")\n",
    "print(f\"  â€¢ Models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy,\")\n",
    "print(f\"           AntiBERTy, BALM, ESM-2, IgBERT, Parapred, SEQID, CDRH3ID\")\n",
    "print(f\"  â€¢ Datasets: SAbDab and DMS\")\n",
    "print(f\"  â€¢ Comparison type: TEST vs TEST (thresholded using VAL vs VAL)\")\n",
    "print(f\"  â€¢ ABodyBuilder2 DTW configurations excluded\")\n",
    "print(f\"  â€¢ Only existing parquet files will be processed\")\n",
    "print(f\"  â€¢ Summary metrics will be {'recalculated' if RECALCULATE_SUMMARYMETRICS else 'reused if available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import typing as T\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import local modules\n",
    "import calculate_metrics\n",
    "import calculate_metrics_dms\n",
    "import models\n",
    "from excel_generator import generate_results_excel, print_summary_stats\n",
    "from ablangpaired_model import AbLangPairedConfig, AbLangPaired\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_file_exists(filepath, description=\"\"):\n",
    "    \"\"\"Check if a file exists and return status.\"\"\"\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = \"âœ…\" if exists else \"âŒ\"\n",
    "    print(f\"  {status} {description}: {filepath}\")\n",
    "    return exists\n",
    "\n",
    "\n",
    "def embed_with_ablangpaired(input_path: str, output_path: str, model_path: str, model_name: str):\n",
    "    \"\"\"Generate embeddings using AbLangPaired models.\n",
    "    \n",
    "        Args:\n",
    "            model_name: str. If \"ablangpre\" then the model architecture will no longer have the mixer layer.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”„ Generating {model_name} embeddings...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_parquet(input_path)\n",
    "    if \"EMBEDDING\" in df.columns:\n",
    "        df = df.drop(columns=[\"EMBEDDING\"])\n",
    "    \n",
    "    # Setup model\n",
    "    model_config = AbLangPairedConfig(checkpoint_filename=model_path)\n",
    "    is_ablangpre = model_name == \"ablangpre\"\n",
    "    model = AbLangPaired(model_config, device=device, use_pretrained=is_ablangpre)\n",
    "    \n",
    "    # Tokenize and embed using enhanced methods\n",
    "    tokenized_dataloader = models.tokenize_data(df, model_config, batch_size=BATCH_SIZE)\n",
    "    all_embeds = models.embed_dataloader(tokenized_dataloader, model, device)\n",
    "    \n",
    "    # Save\n",
    "    df['EMBEDDING'] = list(all_embeds.cpu().numpy())\n",
    "    df.to_parquet(output_path)\n",
    "    \n",
    "    print(f\"âœ… {model_name} embeddings saved to {output_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_summary_file_paths(model_name: str, dataset_type: str, output_folder: str):\n",
    "    \"\"\"Get expected summary file paths for a model/dataset combination.\"\"\"\n",
    "    if dataset_type == \"sabdab\":\n",
    "        # SAbDab has both epitope and antigen summary files\n",
    "        epitope_file = os.path.join(output_folder, f\"{model_name}_sabdab_ep_summarymetrics.txt\")\n",
    "        antigen_file = os.path.join(output_folder, f\"{model_name}_sabdab_ag_summarymetrics.txt\")\n",
    "        return [epitope_file, antigen_file]\n",
    "    elif dataset_type == \"dms\":\n",
    "        # DMS has only one summary file\n",
    "        dms_file = os.path.join(output_folder, f\"{model_name}_dms_summarymetrics.txt\")\n",
    "        return [dms_file]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def read_and_display_summary_results(summary_files: list, model_name: str, dataset_type: str):\n",
    "    \"\"\"Read and display results from existing summary files.\"\"\"\n",
    "    print(f\"\\nðŸ“Š Loading existing results for {model_name} on {dataset_type}:\")\n",
    "    \n",
    "    for summary_file in summary_files:\n",
    "        if os.path.exists(summary_file):\n",
    "            try:\n",
    "                with open(summary_file, 'r') as f:\n",
    "                    content = f.read().strip()\n",
    "                \n",
    "                # Extract the task type from filename\n",
    "                if \"sabdab_ep\" in summary_file:\n",
    "                    task_type = \"Epitope-level performance\"\n",
    "                elif \"sabdab_ag\" in summary_file:\n",
    "                    task_type = \"Antigen-level performance\"\n",
    "                else:\n",
    "                    task_type = \"Performance\"\n",
    "                \n",
    "                print(f\"\\n--- {task_type} ---\")\n",
    "                print(content)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error reading {summary_file}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Summary file not found: {summary_file}\")\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Required Base Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Checking base dataset files...\n",
      "  âœ… SAbDab base dataset: ablangpdb_renameddatasets.parquet\n",
      "  âœ… DMS base dataset: ablangrbd_renameddatasets.parquet\n",
      "  âœ… SAbDab validation labels (test vs test): ablangpdb_val_label_mat.pt\n",
      "  âœ… SAbDab test labels (test vs test): ablangpdb_test_label_mat.pt\n",
      "  âœ… DMS validation labels (test vs test): dms_val_label_mat.pt\n",
      "  âœ… DMS test labels (test vs test): dms_test_label_mat.pt\n",
      "\n",
      "âœ… All base files found!\n"
     ]
    }
   ],
   "source": [
    "# Check that base dataset files exist\n",
    "print(\"ðŸ“‹ Checking base dataset files...\")\n",
    "\n",
    "base_files = {\n",
    "    \"SAbDab base dataset\": \"ablangpdb_renameddatasets.parquet\",\n",
    "    \"DMS base dataset\": \"ablangrbd_renameddatasets.parquet\",\n",
    "    \"SAbDab validation labels (test vs test)\": \"ablangpdb_val_label_mat.pt\",\n",
    "    \"SAbDab test labels (test vs test)\": \"ablangpdb_test_label_mat.pt\",\n",
    "    \"DMS validation labels (test vs test)\": \"dms_val_label_mat.pt\",\n",
    "    \"DMS test labels (test vs test)\": \"dms_test_label_mat.pt\"\n",
    "}\n",
    "\n",
    "missing_base_files = []\n",
    "for desc, filepath in base_files.items():\n",
    "    if not check_file_exists(filepath, desc):\n",
    "        missing_base_files.append(filepath)\n",
    "\n",
    "if missing_base_files:\n",
    "    raise FileNotFoundError(f\"Missing required base files: {missing_base_files}\")\n",
    "\n",
    "print(\"\\nâœ… All base files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EMBEDDING FILE STATUS CHECK\n",
      "============================================================\n",
      " Checking for existing embedding files (no regeneration)...\n",
      "âœ… Found: sabdab_embeddedby_ablangrbd.parquet\n",
      "âœ… Found: sabdab_embeddedby_ablangpre.parquet\n",
      "âœ… Found: sabdab_embeddedby_ablang2.parquet\n",
      "âœ… Found: sabdab_embeddedby_ablang-heavy.parquet\n",
      "âœ… Found: sabdab_embeddedby_antiberty.parquet\n",
      "âœ… Found: sabdab_embeddedby_balm.parquet\n",
      "âœ… Found: sabdab_embeddedby_esm-2.parquet\n",
      "âœ… Found: sabdab_embeddedby_igbert.parquet\n",
      "âœ… Found: sabdab_embeddedby_parapred.parquet\n",
      "âœ… Found: dms_embeddedby_ablangpdb.parquet\n",
      "âœ… Found: dms_embeddedby_ablangpre.parquet\n",
      "âœ… Found: dms_embeddedby_ablang2.parquet\n",
      "âœ… Found: dms_embeddedby_ablang-heavy.parquet\n",
      "âœ… Found: dms_embeddedby_antiberty.parquet\n",
      "âœ… Found: dms_embeddedby_balm.parquet\n",
      "âœ… Found: dms_embeddedby_esm-2.parquet\n",
      "âœ… Found: dms_embeddedby_igbert.parquet\n",
      "âœ… Found: dms_embeddedby_parapred.parquet\n",
      "\n",
      "ðŸ“Š Embedding Files Summary:\n",
      "  â€¢ Existing files: 18/18\n",
      "  â€¢ Missing files: 0\n",
      "\n",
      "âœ… Embedding file check complete!\n"
     ]
    }
   ],
   "source": [
    "# Define all embedding files that should exist\n",
    "embedding_files = {\n",
    "    # SAbDab dataset embeddings\n",
    "    \"sabdab_embeddedby_ablangrbd.parquet\": (\"ablangpdb_renameddatasets.parquet\", MODEL_PATHS.get(\"AbLangRBD\"), \"AbLangRBD\"),\n",
    "    \"sabdab_embeddedby_ablangpre.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AbLangPre\"),\n",
    "    \"sabdab_embeddedby_ablang2.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AbLang2\"),\n",
    "    \"sabdab_embeddedby_ablang-heavy.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AbLang-Heavy\"),\n",
    "    \"sabdab_embeddedby_antiberty.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AntiBERTy\"),\n",
    "    \"sabdab_embeddedby_balm.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"BALM\"),\n",
    "    \"sabdab_embeddedby_esm-2.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"ESM-2\"),\n",
    "    \"sabdab_embeddedby_igbert.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"IgBERT\"),\n",
    "    \"sabdab_embeddedby_parapred.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"Parapred\"),\n",
    "\n",
    "    # DMS dataset embeddings\n",
    "    \"dms_embeddedby_ablangpdb.parquet\": (\"ablangrbd_renameddatasets.parquet\", MODEL_PATHS.get(\"AbLangPDB\"), \"AbLangPDB\"),\n",
    "    \"dms_embeddedby_ablangpre.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AbLangPre\"),\n",
    "    \"dms_embeddedby_ablang2.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AbLang2\"),\n",
    "    \"dms_embeddedby_ablang-heavy.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AbLang-Heavy\"),\n",
    "    \"dms_embeddedby_antiberty.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AntiBERTy\"),\n",
    "    \"dms_embeddedby_balm.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"BALM\"),\n",
    "    \"dms_embeddedby_esm-2.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"ESM-2\"),\n",
    "    \"dms_embeddedby_igbert.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"IgBERT\"),\n",
    "    \"dms_embeddedby_parapred.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"Parapred\")\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EMBEDDING FILE STATUS CHECK\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\" Checking for existing embedding files (no regeneration)...\")\n",
    "\n",
    "existing_files = []\n",
    "missing_files = []\n",
    "\n",
    "for output_file, (input_file, model_path, model_name) in embedding_files.items():\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"âœ… Found: {output_file}\")\n",
    "        existing_files.append(output_file)\n",
    "    else:\n",
    "        print(f\"âŒ Missing: {output_file}\")\n",
    "        missing_files.append(output_file)\n",
    "\n",
    "print(f\"\\nðŸ“Š Embedding Files Summary:\")\n",
    "print(f\"  â€¢ Existing files: {len(existing_files)}/{len(embedding_files)}\")\n",
    "print(f\"  â€¢ Missing files: {len(missing_files)}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâš ï¸ Missing embedding files:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"  - {file}\")\n",
    "    print(\"\\nNote: Configurations using missing files will be skipped automatically.\")\n",
    "\n",
    "print(f\"\\nâœ… Embedding file check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify All Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Verifying all required files...\n",
      "  âœ… SAbDab base (AbLangPDB embeddings): ablangpdb_renameddatasets.parquet\n",
      "  âœ… DMS base: ablangrbd_renameddatasets.parquet\n",
      "  âœ… SAbDab + AbLangRBD: sabdab_embeddedby_ablangrbd.parquet\n",
      "  âœ… SAbDab + AbLangPre: sabdab_embeddedby_ablangpre.parquet\n",
      "  âœ… SAbDab + AbLang2: sabdab_embeddedby_ablang2.parquet\n",
      "  âœ… SAbDab + AbLang-Heavy: sabdab_embeddedby_ablang-heavy.parquet\n",
      "  âœ… SAbDab + AntiBERTy: sabdab_embeddedby_antiberty.parquet\n",
      "  âœ… SAbDab + BALM: sabdab_embeddedby_balm.parquet\n",
      "  âœ… SAbDab + ESM-2: sabdab_embeddedby_esm-2.parquet\n",
      "  âœ… SAbDab + IgBERT: sabdab_embeddedby_igbert.parquet\n",
      "  âœ… SAbDab + Parapred: sabdab_embeddedby_parapred.parquet\n",
      "  âœ… DMS + AbLangPDB: dms_embeddedby_ablangpdb.parquet\n",
      "  âœ… DMS + AbLangPre: dms_embeddedby_ablangpre.parquet\n",
      "  âœ… DMS + AbLang2: dms_embeddedby_ablang2.parquet\n",
      "  âœ… DMS + AbLang-Heavy: dms_embeddedby_ablang-heavy.parquet\n",
      "  âœ… DMS + AntiBERTy: dms_embeddedby_antiberty.parquet\n",
      "  âœ… DMS + BALM: dms_embeddedby_balm.parquet\n",
      "  âœ… DMS + ESM-2: dms_embeddedby_esm-2.parquet\n",
      "  âœ… DMS + IgBERT: dms_embeddedby_igbert.parquet\n",
      "  âœ… DMS + Parapred: dms_embeddedby_parapred.parquet\n",
      "  âœ… SAbDab validation labels (test vs test): ablangpdb_val_label_mat.pt\n",
      "  âœ… SAbDab test labels (test vs test): ablangpdb_test_label_mat.pt\n",
      "  âœ… DMS validation labels (test vs test): dms_val_label_mat.pt\n",
      "  âœ… DMS test labels (test vs test): dms_test_label_mat.pt\n",
      "\n",
      "âœ… All required files are available!\n"
     ]
    }
   ],
   "source": [
    "# Check that all required files now exist\n",
    "print(\"\\nðŸ“‹ Verifying all required files...\")\n",
    "\n",
    "all_required_files = {\n",
    "    # Base datasets\n",
    "    \"SAbDab base (AbLangPDB embeddings)\": \"ablangpdb_renameddatasets.parquet\",\n",
    "    \"DMS base\": \"ablangrbd_renameddatasets.parquet\",\n",
    "    \n",
    "    # SAbDab embedding files\n",
    "    \"SAbDab + AbLangRBD\": \"sabdab_embeddedby_ablangrbd.parquet\",\n",
    "    \"SAbDab + AbLangPre\": \"sabdab_embeddedby_ablangpre.parquet\",\n",
    "    \"SAbDab + AbLang2\": \"sabdab_embeddedby_ablang2.parquet\",\n",
    "    \"SAbDab + AbLang-Heavy\": \"sabdab_embeddedby_ablang-heavy.parquet\",\n",
    "    \"SAbDab + AntiBERTy\": \"sabdab_embeddedby_antiberty.parquet\",\n",
    "    \"SAbDab + BALM\": \"sabdab_embeddedby_balm.parquet\",\n",
    "    \"SAbDab + ESM-2\": \"sabdab_embeddedby_esm-2.parquet\",\n",
    "    \"SAbDab + IgBERT\": \"sabdab_embeddedby_igbert.parquet\",\n",
    "    \"SAbDab + Parapred\": \"sabdab_embeddedby_parapred.parquet\",\n",
    "    \n",
    "    # DMS embedding files\n",
    "    \"DMS + AbLangPDB\": \"dms_embeddedby_ablangpdb.parquet\",\n",
    "    \"DMS + AbLangPre\": \"dms_embeddedby_ablangpre.parquet\",\n",
    "    \"DMS + AbLang2\": \"dms_embeddedby_ablang2.parquet\",\n",
    "    \"DMS + AbLang-Heavy\": \"dms_embeddedby_ablang-heavy.parquet\",\n",
    "    \"DMS + AntiBERTy\": \"dms_embeddedby_antiberty.parquet\",\n",
    "    \"DMS + BALM\": \"dms_embeddedby_balm.parquet\",\n",
    "    \"DMS + ESM-2\": \"dms_embeddedby_esm-2.parquet\",\n",
    "    \"DMS + IgBERT\": \"dms_embeddedby_igbert.parquet\",\n",
    "    \"DMS + Parapred\": \"dms_embeddedby_parapred.parquet\",\n",
    "    \n",
    "    # Label matrices (test vs test)\n",
    "    \"SAbDab validation labels (test vs test)\": \"ablangpdb_val_label_mat.pt\",\n",
    "    \"SAbDab test labels (test vs test)\": \"ablangpdb_test_label_mat.pt\",\n",
    "    \"DMS validation labels (test vs test)\": \"dms_val_label_mat.pt\",\n",
    "    \"DMS test labels (test vs test)\": \"dms_test_label_mat.pt\"\n",
    "}\n",
    "\n",
    "missing_files = []\n",
    "for desc, filepath in all_required_files.items():\n",
    "    if not check_file_exists(filepath, desc):\n",
    "        missing_files.append(filepath)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâš ï¸ Warning: {len(missing_files)} files are missing:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"  - {file}\")\n",
    "    print(\"\\nProceeding with available files only.\")\n",
    "else:\n",
    "    print(\"\\nâœ… All required files are available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 24 model/dataset/metric combinations:\n",
      "  â€¢ ablangpdb_sabdab_cosine: AbLangPDB on sabdab using cosine\n",
      "  â€¢ ablangrbd_sabdab_cosine: AbLangRBD on sabdab using cosine\n",
      "  â€¢ ablangpre_sabdab_cosine: AbLangPre on sabdab using cosine\n",
      "  â€¢ ablang2_sabdab_cosine: AbLang2 on sabdab using cosine\n",
      "  â€¢ ablang_heavy_sabdab_cosine: AbLang-Heavy on sabdab using cosine\n",
      "  â€¢ antiberty_sabdab_cosine: AntiBERTy on sabdab using cosine\n",
      "  â€¢ balm_sabdab_cosine: BALM on sabdab using cosine\n",
      "  â€¢ esm2_sabdab_cosine: ESM-2 on sabdab using cosine\n",
      "  â€¢ igbert_sabdab_cosine: IgBERT on sabdab using cosine\n",
      "  â€¢ parapred_sabdab_cosine: Parapred on sabdab using cosine\n",
      "  â€¢ seqid_sabdab: SEQID on sabdab using seq_identity\n",
      "  â€¢ cdrh3id_sabdab: CDRH3ID on sabdab using cdrh3_identity\n",
      "  â€¢ ablangpdb_dms_cosine: AbLangPDB on dms using cosine\n",
      "  â€¢ ablangrbd_dms_cosine: AbLangRBD on dms using cosine\n",
      "  â€¢ ablangpre_dms_cosine: AbLangPre on dms using cosine\n",
      "  â€¢ ablang2_dms_cosine: AbLang2 on dms using cosine\n",
      "  â€¢ ablang_heavy_dms_cosine: AbLang-Heavy on dms using cosine\n",
      "  â€¢ antiberty_dms_cosine: AntiBERTy on dms using cosine\n",
      "  â€¢ balm_dms_cosine: BALM on dms using cosine\n",
      "  â€¢ esm2_dms_cosine: ESM-2 on dms using cosine\n",
      "  â€¢ igbert_dms_cosine: IgBERT on dms using cosine\n",
      "  â€¢ parapred_dms_cosine: Parapred on dms using cosine\n",
      "  â€¢ seqid_dms: SEQID on dms using seq_identity\n",
      "  â€¢ cdrh3id_dms: CDRH3ID on dms using cdrh3_identity\n",
      "\n",
      "ðŸ“ Note: ABodyBuilder2 DTW configurations have been excluded from this analysis\n"
     ]
    }
   ],
   "source": [
    "# Complete configuration for all model/dataset/metric combinations\n",
    "CONFIGS = {\n",
    "    # SAbDab Dataset Configurations\n",
    "    \"ablangpdb_sabdab_cosine\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPDB\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablangrbd_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablangrbd.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangRBD\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablangpre_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablangpre.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPre\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablang2_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablang2.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLang2\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablang_heavy_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablang-heavy.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLang-Heavy\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"antiberty_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_antiberty.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AntiBERTy\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"balm_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_balm.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"BALM\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"esm2_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_esm-2.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"ESM-2\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"igbert_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_igbert.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"IgBERT\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"parapred_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_parapred.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"Parapred\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"seqid_sabdab\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"SEQID\",\n",
    "        \"score_type\": \"seq_identity\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"cdrh3id_sabdab\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"CDRH3ID\",\n",
    "        \"score_type\": \"cdrh3_identity\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \n",
    "    # DMS Dataset Configurations\n",
    "    \"ablangpdb_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablangpdb.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPDB\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablangrbd_dms_cosine\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangRBD\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablangpre_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablangpre.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPre\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablang2_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablang2.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLang2\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablang_heavy_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablang-heavy.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLang-Heavy\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"antiberty_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_antiberty.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AntiBERTy\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"balm_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_balm.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"BALM\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"esm2_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_esm-2.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"ESM-2\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"igbert_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_igbert.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"IgBERT\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"parapred_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_parapred.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"Parapred\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"seqid_dms\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"SEQID\",\n",
    "        \"score_type\": \"seq_identity\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"cdrh3id_dms\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"CDRH3ID\",\n",
    "        \"score_type\": \"cdrh3_identity\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(CONFIGS)} model/dataset/metric combinations:\")\n",
    "for name, config in CONFIGS.items():\n",
    "    print(f\"  â€¢ {name}: {config['model_name']} on {config['dataset_type']} using {config['score_type']}\")\n",
    "    \n",
    "print(f\"\\nðŸ“ Note: ABodyBuilder2 DTW configurations have been excluded from this analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Available Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ablangpdb_sabdab_cosine: Ready to run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ablangrbd_sabdab_cosine: Ready to run\n",
      "âœ… ablangpre_sabdab_cosine: Ready to run\n",
      "âœ… ablang2_sabdab_cosine: Ready to run\n",
      "âœ… ablang_heavy_sabdab_cosine: Ready to run\n",
      "âœ… antiberty_sabdab_cosine: Ready to run\n",
      "âœ… balm_sabdab_cosine: Ready to run\n",
      "âœ… esm2_sabdab_cosine: Ready to run\n",
      "âœ… igbert_sabdab_cosine: Ready to run\n",
      "âœ… parapred_sabdab_cosine: Ready to run\n",
      "âœ… seqid_sabdab: Ready to run\n",
      "âœ… cdrh3id_sabdab: Ready to run\n",
      "âœ… ablangpdb_dms_cosine: Ready to run\n",
      "âœ… ablangrbd_dms_cosine: Ready to run\n",
      "âœ… ablangpre_dms_cosine: Ready to run\n",
      "âœ… ablang2_dms_cosine: Ready to run\n",
      "âœ… ablang_heavy_dms_cosine: Ready to run\n",
      "âœ… antiberty_dms_cosine: Ready to run\n",
      "âœ… balm_dms_cosine: Ready to run\n",
      "âœ… esm2_dms_cosine: Ready to run\n",
      "âœ… igbert_dms_cosine: Ready to run\n",
      "âœ… parapred_dms_cosine: Ready to run\n",
      "âœ… seqid_dms: Ready to run\n",
      "âœ… cdrh3id_dms: Ready to run\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "  â€¢ Available configurations: 24/24\n",
      "  â€¢ Missing configurations: 0\n"
     ]
    }
   ],
   "source": [
    "# Check which configurations can actually run based on available files\n",
    "available_configs = {}\n",
    "missing_configs = []\n",
    "\n",
    "for config_name, config in CONFIGS.items():\n",
    "    files_to_check = [config[\"df_path\"], config[\"labels_val\"], config[\"labels_test\"]]\n",
    "    missing_files = [f for f in files_to_check if not os.path.exists(f)]\n",
    "    \n",
    "    if not missing_files:\n",
    "        available_configs[config_name] = config\n",
    "        print(f\"âœ… {config_name}: Ready to run\")\n",
    "    else:\n",
    "        missing_configs.append(config_name)\n",
    "        print(f\"âŒ {config_name}: Missing files - {missing_files}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"  â€¢ Available configurations: {len(available_configs)}/{len(CONFIGS)}\")\n",
    "print(f\"  â€¢ Missing configurations: {len(missing_configs)}\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"\\nâš ï¸ Configurations that will be skipped: {', '.join(missing_configs)}\")\n",
    "\n",
    "if not available_configs:\n",
    "    raise RuntimeError(\"âŒ No configurations are available to run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Pre-computed Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ†• Computing fresh thresholds for all configurations\n",
      "ðŸ“ Note: Test vs test comparisons require new threshold optimization\n"
     ]
    }
   ],
   "source": [
    "# Optional: Pre-computed thresholds to skip threshold optimization\n",
    "# NOTE: Thresholds from train vs test analysis are not valid for test vs test comparisons\n",
    "# These thresholds need to be recalculated for the new comparison type\n",
    "\n",
    "PRECOMPUTED_THRESHOLDS = {\n",
    "    # Thresholds will need to be recalculated for test vs test comparisons\n",
    "    # The original thresholds were optimized for train vs test, not test vs test\n",
    "    # Leave empty to force recalculation\n",
    "}\n",
    "\n",
    "use_precomputed = len(PRECOMPUTED_THRESHOLDS) > 0\n",
    "if use_precomputed:\n",
    "    print(f\"ðŸ”„ Using precomputed thresholds for {len(PRECOMPUTED_THRESHOLDS)} configurations\")\n",
    "    for config_name, thresholds in PRECOMPUTED_THRESHOLDS.items():\n",
    "        print(f\"  â€¢ {config_name}: {thresholds}\")\n",
    "else:\n",
    "    print(\"ðŸ†• Computing fresh thresholds for all configurations\")\n",
    "    print(\"ðŸ“ Note: Test vs test comparisons require new threshold optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "DATASET",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ef0161e9-02df-4b90-ada4-c6cf6ee645df",
       "rows": [
        [
         "TRAIN",
         "1517"
        ],
        [
         "TEST",
         "202"
        ],
        [
         "VAL",
         "190"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "DATASET\n",
       "TRAIN    1517\n",
       "TEST      202\n",
       "VAL       190\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('ablangpdb_renameddatasets.parquet')\n",
    "df[\"DATASET\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comprehensive Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE BENCHMARK EXECUTION\n",
      "======================================================================\n",
      "Total configurations to run: 24\n",
      "Recalculate summary metrics: False\n",
      "\n",
      "======================================================================\n",
      "[1/24] Running: ablangpdb_sabdab_cosine\n",
      "Model: AbLangPDB, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.4388\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.2364\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6768, Average Precision: 0.3275, F1 Score: 0.3503\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6800, Average Precision: 0.3360, F1 Score: 0.3259\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangPDB_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLangPDB_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [1/24] ablangpdb_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[2/24] Running: ablangrbd_sabdab_cosine\n",
      "Model: AbLangRBD, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.7961\n",
      "Optimal F1 Threshold for Antigen (>=0.2): -0.1440\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6138, Average Precision: 0.2466, F1 Score: 0.2226\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5301, Average Precision: 0.2063, F1 Score: 0.1874\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangRBD_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLangRBD_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [2/24] ablangrbd_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[3/24] Running: ablangpre_sabdab_cosine\n",
      "Model: AbLangPre, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.8113\n",
      "Optimal F1 Threshold for Antigen (>=0.2): -0.0926\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6804, Average Precision: 0.2111, F1 Score: 0.1876\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6315, Average Precision: 0.2472, F1 Score: 0.1979\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangPre_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLangPre_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [3/24] ablangpre_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[4/24] Running: ablang2_sabdab_cosine\n",
      "Model: AbLang2, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9362\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.1976\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.5552, Average Precision: 0.0828, F1 Score: 0.1153\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5339, Average Precision: 0.1452, F1 Score: 0.1980\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLang2_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLang2_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [4/24] ablang2_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[5/24] Running: ablang_heavy_sabdab_cosine\n",
      "Model: AbLang-Heavy, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.8132\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.6648\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6799, Average Precision: 0.1949, F1 Score: 0.2341\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6423, Average Precision: 0.2329, F1 Score: 0.2615\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLang-Heavy_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLang-Heavy_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [5/24] ablang_heavy_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[6/24] Running: antiberty_sabdab_cosine\n",
      "Model: AntiBERTy, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.8221\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.6469\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.7052, Average Precision: 0.2640, F1 Score: 0.2991\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6596, Average Precision: 0.2654, F1 Score: 0.2509\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AntiBERTy_sabdab_ep_summarymetrics.txt and output_csvs_test/AntiBERTy_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [6/24] antiberty_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[7/24] Running: balm_sabdab_cosine\n",
      "Model: BALM, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9446\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.9226\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6334, Average Precision: 0.1882, F1 Score: 0.1879\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5979, Average Precision: 0.2157, F1 Score: 0.2406\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/BALM_sabdab_ep_summarymetrics.txt and output_csvs_test/BALM_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [7/24] balm_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[8/24] Running: esm2_sabdab_cosine\n",
      "Model: ESM-2, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9953\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.9902\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6047, Average Precision: 0.1721, F1 Score: 0.2039\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5611, Average Precision: 0.1870, F1 Score: 0.1965\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/ESM-2_sabdab_ep_summarymetrics.txt and output_csvs_test/ESM-2_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [8/24] esm2_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[9/24] Running: igbert_sabdab_cosine\n",
      "Model: IgBERT, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9752\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.9655\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6272, Average Precision: 0.1382, F1 Score: 0.1837\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6011, Average Precision: 0.2007, F1 Score: 0.2316\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/IgBERT_sabdab_ep_summarymetrics.txt and output_csvs_test/IgBERT_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [9/24] igbert_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[10/24] Running: parapred_sabdab_cosine\n",
      "Model: Parapred, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9985\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.9981\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6320, Average Precision: 0.1813, F1 Score: 0.1465\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5889, Average Precision: 0.1958, F1 Score: 0.2128\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/Parapred_sabdab_ep_summarymetrics.txt and output_csvs_test/Parapred_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [10/24] parapred_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[11/24] Running: seqid_sabdab\n",
      "Model: SEQID, Dataset: sabdab, Score: seq_identity\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'seq_identity' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.5576\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.3054\n",
      "Preparing data with score_type: 'seq_identity' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6876, Average Precision: 0.2628, F1 Score: 0.1259\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5832, Average Precision: 0.2291, F1 Score: 0.1979\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/SEQID_sabdab_ep_summarymetrics.txt and output_csvs_test/SEQID_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [11/24] seqid_sabdab completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[12/24] Running: cdrh3id_sabdab\n",
      "Model: CDRH3ID, Dataset: sabdab, Score: cdrh3_identity\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cdrh3_identity' for VAL vs VAL...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.3810\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.0000\n",
      "Preparing data with score_type: 'cdrh3_identity' for TEST vs TEST...\n",
      "  ðŸ”„ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  ðŸ“Š Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6390, Average Precision: 0.2341, F1 Score: 0.1741\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5415, Average Precision: 0.1978, F1 Score: 0.1979\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/CDRH3ID_sabdab_ep_summarymetrics.txt and output_csvs_test/CDRH3ID_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "âœ… [12/24] cdrh3id_sabdab completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[13/24] Running: ablangpdb_dms_cosine\n",
      "Model: AbLangPDB, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: -0.0278\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5348, Average Precision: 0.1375, F1 Score: 0.1979\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangPDB_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [13/24] ablangpdb_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[14/24] Running: ablangrbd_dms_cosine\n",
      "Model: AbLangRBD, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.7934\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.7279, Average Precision: 0.3944, F1 Score: 0.3859\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangRBD_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [14/24] ablangrbd_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[15/24] Running: ablangpre_dms_cosine\n",
      "Model: AbLangPre, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6833\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5698, Average Precision: 0.1626, F1 Score: 0.1965\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangPre_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [15/24] ablangpre_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[16/24] Running: ablang2_dms_cosine\n",
      "Model: AbLang2, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.5164\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5277, Average Precision: 0.1206, F1 Score: 0.1978\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLang2_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [16/24] ablang2_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[17/24] Running: ablang_heavy_dms_cosine\n",
      "Model: AbLang-Heavy, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6799\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5603, Average Precision: 0.1545, F1 Score: 0.1980\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLang-Heavy_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [17/24] ablang_heavy_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[18/24] Running: antiberty_dms_cosine\n",
      "Model: AntiBERTy, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6837\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5675, Average Precision: 0.1610, F1 Score: 0.2060\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AntiBERTy_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [18/24] antiberty_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[19/24] Running: balm_dms_cosine\n",
      "Model: BALM, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.9387\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5545, Average Precision: 0.1498, F1 Score: 0.2002\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/BALM_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [19/24] balm_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[20/24] Running: esm2_dms_cosine\n",
      "Model: ESM-2, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.9933\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5576, Average Precision: 0.1650, F1 Score: 0.1966\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/ESM-2_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [20/24] esm2_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[21/24] Running: igbert_dms_cosine\n",
      "Model: IgBERT, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6031\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5373, Average Precision: 0.1328, F1 Score: 0.1969\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/IgBERT_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [21/24] igbert_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[22/24] Running: parapred_dms_cosine\n",
      "Model: Parapred, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.9909\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5516, Average Precision: 0.1438, F1 Score: 0.1969\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/Parapred_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [22/24] parapred_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[23/24] Running: seqid_dms\n",
      "Model: SEQID, Dataset: dms, Score: seq_identity\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'seq_identity' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6708\n",
      "Preparing data with score_type: 'seq_identity' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5501, Average Precision: 0.1765, F1 Score: 0.1918\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/SEQID_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [23/24] seqid_dms completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[24/24] Running: cdrh3id_dms\n",
      "Model: CDRH3ID, Dataset: dms, Score: cdrh3_identity\n",
      "======================================================================\n",
      "ðŸ“‹ Some summary files missing, proceeding with calculation...\n",
      "ðŸ”„ Running with parameters:\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Final evaluation: TEST vs TEST\n",
      "  â€¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cdrh3_identity' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.1905\n",
      "Preparing data with score_type: 'cdrh3_identity' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5273, Average Precision: 0.1285, F1 Score: 0.1971\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/CDRH3ID_dms_summarymetrics.txt\n",
      "\n",
      "âœ… [24/24] cdrh3id_dms completed successfully!\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE BENCHMARK EXECUTION SUMMARY\n",
      "======================================================================\n",
      "ablangpdb_sabdab_cosine        âœ… Success\n",
      "ablangrbd_sabdab_cosine        âœ… Success\n",
      "ablangpre_sabdab_cosine        âœ… Success\n",
      "ablang2_sabdab_cosine          âœ… Success\n",
      "ablang_heavy_sabdab_cosine     âœ… Success\n",
      "antiberty_sabdab_cosine        âœ… Success\n",
      "balm_sabdab_cosine             âœ… Success\n",
      "esm2_sabdab_cosine             âœ… Success\n",
      "igbert_sabdab_cosine           âœ… Success\n",
      "parapred_sabdab_cosine         âœ… Success\n",
      "seqid_sabdab                   âœ… Success\n",
      "cdrh3id_sabdab                 âœ… Success\n",
      "ablangpdb_dms_cosine           âœ… Success\n",
      "ablangrbd_dms_cosine           âœ… Success\n",
      "ablangpre_dms_cosine           âœ… Success\n",
      "ablang2_dms_cosine             âœ… Success\n",
      "ablang_heavy_dms_cosine        âœ… Success\n",
      "antiberty_dms_cosine           âœ… Success\n",
      "balm_dms_cosine                âœ… Success\n",
      "esm2_dms_cosine                âœ… Success\n",
      "igbert_dms_cosine              âœ… Success\n",
      "parapred_dms_cosine            âœ… Success\n",
      "seqid_dms                      âœ… Success\n",
      "cdrh3id_dms                    âœ… Success\n",
      "\n",
      "ðŸ“Š Results:\n",
      "  â€¢ Successful: 24/24\n",
      "  â€¢ Failed: 0\n",
      "\n",
      "ðŸŽ‰ All available configurations completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Execute all available configurations\n",
    "execution_results = {}\n",
    "failed_configs = []\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"COMPREHENSIVE BENCHMARK EXECUTION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total configurations to run: {len(available_configs)}\")\n",
    "print(f\"Recalculate summary metrics: {RECALCULATE_SUMMARYMETRICS}\")\n",
    "\n",
    "for i, (config_name, config) in enumerate(available_configs.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i}/{len(available_configs)}] Running: {config_name}\")\n",
    "    print(f\"Model: {config['model_name']}, Dataset: {config['dataset_type']}, Score: {config['score_type']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check if summary files already exist and flag is False\n",
    "    if not RECALCULATE_SUMMARYMETRICS:\n",
    "        summary_files = get_summary_file_paths(config['model_name'], config['dataset_type'], OUTPUT_FOLDER)\n",
    "        all_summaries_exist = all(os.path.exists(f) for f in summary_files)\n",
    "        \n",
    "        if all_summaries_exist:\n",
    "            print(f\"ðŸ“‹ Summary files already exist, skipping recalculation...\")\n",
    "            read_and_display_summary_results(summary_files, config['model_name'], config['dataset_type'])\n",
    "            execution_results[config_name] = \"âœ… Loaded from existing files\"\n",
    "            print(f\"\\nâœ… [{i}/{len(available_configs)}] {config_name} loaded from existing files!\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"ðŸ“‹ Some summary files missing, proceeding with calculation...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare arguments for TEST vs TEST comparisons\n",
    "        args = {\n",
    "            \"df_path\": config[\"df_path\"],\n",
    "            \"labels_file_val\": config[\"labels_val\"],\n",
    "            \"labels_file_test\": config[\"labels_test\"],\n",
    "            \"score_type\": config[\"score_type\"],\n",
    "            \"model_name\": config[\"model_name\"],\n",
    "            \"output_folder\": OUTPUT_FOLDER,\n",
    "            # CRITICAL FIX: Use correct dataset parameters for test vs test\n",
    "            \"dataset1\": \"VAL\",           # Use VAL for threshold optimization (VAL vs VAL)\n",
    "            \"dataset2_val\": \"VAL\",       # VAL vs VAL for F1 threshold finding\n",
    "            \"dataset2_test\": \"TEST\",     # TEST vs TEST for final evaluation\n",
    "            \"use_square_matrices\": True  # Enable proper square matrix handling\n",
    "        }\n",
    "        \n",
    "        # Add matrix file paths for ABodyBuilder2 DTW configurations (if they exist)\n",
    "        if config[\"score_type\"] == \"abodybuilder2_dtw_cdrs\":\n",
    "            args[\"matrix_file_val\"] = config[\"matrix_file_val\"]\n",
    "            args[\"matrix_file_test\"] = config[\"matrix_file_test\"]\n",
    "        \n",
    "        # Add precomputed thresholds if available\n",
    "        if config_name in PRECOMPUTED_THRESHOLDS:\n",
    "            thresholds = PRECOMPUTED_THRESHOLDS[config_name]\n",
    "            if config[\"dataset_type\"] == \"sabdab\":\n",
    "                if \"epitope_threshold\" in thresholds:\n",
    "                    args[\"epitope_threshold\"] = thresholds[\"epitope_threshold\"]\n",
    "                if \"antigen_threshold\" in thresholds:\n",
    "                    args[\"antigen_threshold\"] = thresholds[\"antigen_threshold\"]\n",
    "            elif config[\"dataset_type\"] == \"dms\":\n",
    "                if \"epitope_threshold\" in thresholds:\n",
    "                    args[\"epitope_threshold\"] = thresholds[\"epitope_threshold\"]\n",
    "        \n",
    "        # Execute the benchmark\n",
    "        print(f\"ðŸ”„ Running with parameters:\")\n",
    "        print(f\"  â€¢ Threshold optimization: VAL vs VAL\")\n",
    "        print(f\"  â€¢ Final evaluation: TEST vs TEST\")\n",
    "        print(f\"  â€¢ Square matrix mode: ENABLED\")\n",
    "        \n",
    "        config[\"function\"](**args)\n",
    "        \n",
    "        execution_results[config_name] = \"âœ… Success\"\n",
    "        print(f\"\\nâœ… [{i}/{len(available_configs)}] {config_name} completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"âŒ Error: {str(e)}\"\n",
    "        execution_results[config_name] = error_msg\n",
    "        failed_configs.append(config_name)\n",
    "        print(f\"\\nâŒ [{i}/{len(available_configs)}] {config_name} failed: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPREHENSIVE BENCHMARK EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for config_name, result in execution_results.items():\n",
    "    print(f\"{config_name:30} {result}\")\n",
    "\n",
    "successful_configs = len(available_configs) - len(failed_configs)\n",
    "print(f\"\\nðŸ“Š Results:\")\n",
    "print(f\"  â€¢ Successful: {successful_configs}/{len(available_configs)}\")\n",
    "print(f\"  â€¢ Failed: {len(failed_configs)}\")\n",
    "\n",
    "if failed_configs:\n",
    "    print(f\"\\nâš ï¸ Failed configurations: {', '.join(failed_configs)}\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ All available configurations completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Excel Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Generating summary statistics...\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Total summary files found: 36\n",
      "Unique models: 12 (AbLang-Heavy, AbLang2, AbLangPDB, AbLangPre, AbLangRBD, AntiBERTy, BALM, CDRH3ID, ESM-2, IgBERT, Parapred, SEQID)\n",
      "Unique datasets: 3 (dms, sabdab_ag, sabdab_ep)\n",
      "Unique score types: 3 (cdrh3_identity, cosine, seq_identity)\n",
      "Best ROC_AUC: AbLangRBD on dms (0.7279)\n",
      "Best Average_Precision: AbLangRBD on dms (0.3944)\n",
      "Best F1_Score: AbLangRBD on dms (0.3859)\n",
      "\n",
      "======================================================================\n",
      "GENERATING COMPREHENSIVE EXCEL REPORT\n",
      "======================================================================\n",
      "Collecting summary metrics from output_csvs_test...\n",
      "Found 36 summary files\n",
      "Creating pivot table...\n",
      "Pivot table created with 12 models and 9 metric columns\n",
      "Ranking values for formatting...\n",
      "Exporting to Excel: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "âœ… Excel file generated successfully: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "\n",
      "ðŸŽ‰ Comprehensive Excel report generated successfully!\n",
      "ðŸ“ File location: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "ðŸ“ File size: 5,887 bytes\n",
      "\n",
      "ðŸ“– Excel Report Contents:\n",
      "  â€¢ Models as rows (AbLangPDB, AbLangRBD, AbLangPre, SEQID, CDRH3ID)\n",
      "  â€¢ Datasets grouped as column headers (SAbDab, DMS)\n",
      "  â€¢ Metrics: ROC-AUC, Average Precision, F1 Score\n",
      "  â€¢ Best performance: Bold formatting\n",
      "  â€¢ Second best: Italic formatting\n",
      "  â€¢ Values rounded to 4 decimal places\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics\n",
    "print(\"\\nðŸ“Š Generating summary statistics...\")\n",
    "print_summary_stats(OUTPUT_FOLDER)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING COMPREHENSIVE EXCEL REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Generate the Excel file\n",
    "    excel_path = generate_results_excel(\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        excel_filename=EXCEL_FILENAME\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Comprehensive Excel report generated successfully!\")\n",
    "    print(f\"ðŸ“ File location: {excel_path}\")\n",
    "    print(f\"ðŸ“ File size: {os.path.getsize(excel_path):,} bytes\")\n",
    "    \n",
    "    # Provide usage instructions\n",
    "    print(f\"\\nðŸ“– Excel Report Contents:\")\n",
    "    print(f\"  â€¢ Models as rows (AbLangPDB, AbLangRBD, AbLangPre, SEQID, CDRH3ID)\")\n",
    "    print(f\"  â€¢ Datasets grouped as column headers (SAbDab, DMS)\")\n",
    "    print(f\"  â€¢ Metrics: ROC-AUC, Average Precision, F1 Score\")\n",
    "    print(f\"  â€¢ Best performance: Bold formatting\")\n",
    "    print(f\"  â€¢ Second best: Italic formatting\")\n",
    "    print(f\"  â€¢ Values rounded to 4 decimal places\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error generating Excel report: {str(e)}\")\n",
    "    print(\"\\nDebugging information:\")\n",
    "    print(f\"  â€¢ Output folder: {OUTPUT_FOLDER}\")\n",
    "    print(f\"  â€¢ Files in folder: {len(os.listdir(OUTPUT_FOLDER))}\")\n",
    "    \n",
    "    # List summary files found\n",
    "    import glob\n",
    "    summary_files = glob.glob(os.path.join(OUTPUT_FOLDER, \"*summarymetrics.txt\"))\n",
    "    print(f\"  â€¢ Summary files found: {len(summary_files)}\")\n",
    "    for f in summary_files[:5]:  # Show first 5\n",
    "        print(f\"    - {os.path.basename(f)}\")\n",
    "    if len(summary_files) > 5:\n",
    "        print(f\"    - ... and {len(summary_files)-5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST vs TEST PIPELINE COMPLETION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ”§ Configuration:\n",
      "  â€¢ Analysis type: TEST vs TEST comparisons\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Recalculated embeddings: False\n",
      "  â€¢ Used precomputed thresholds: False\n",
      "  â€¢ Batch size: 256\n",
      "\n",
      "ðŸ“ˆ Benchmarking Results:\n",
      "  â€¢ Total configurations possible: 24\n",
      "  â€¢ Configurations attempted: 24\n",
      "  â€¢ Successful runs: 24\n",
      "  â€¢ Failed runs: 0\n",
      "\n",
      "ðŸ“Š Excel Report:\n",
      "  â€¢ Status: âœ… Generated successfully\n",
      "  â€¢ Location: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "  â€¢ Ready for analysis and sharing\n",
      "\n",
      "ðŸ”¬ Models Configured for Benchmarking:\n",
      "  â€¢ AbLang-Heavy (cosine)\n",
      "  â€¢ AbLang2 (cosine)\n",
      "  â€¢ AbLangPDB (cosine)\n",
      "  â€¢ AbLangPre (cosine)\n",
      "  â€¢ AbLangRBD (cosine)\n",
      "  â€¢ AntiBERTy (cosine)\n",
      "  â€¢ BALM (cosine)\n",
      "  â€¢ CDRH3ID (cdrh3_identity)\n",
      "  â€¢ ESM-2 (cosine)\n",
      "  â€¢ IgBERT (cosine)\n",
      "  â€¢ Parapred (cosine)\n",
      "  â€¢ SEQID (seq_identity)\n",
      "\n",
      "ðŸ“Š Datasets Configured:\n",
      "  â€¢ DMS\n",
      "  â€¢ SABDAB\n",
      "\n",
      "ðŸŽ¯ Key Differences from Train vs Test Analysis:\n",
      "  1. ðŸ”„ Compares TEST antibodies against other TEST antibodies\n",
      "  2. ðŸŽ¯ Uses VAL vs VAL for F1 threshold optimization\n",
      "  3. ðŸ“Š Uses test_label_mat.pt and val_label_mat.pt matrices\n",
      "  4. ðŸš« Excludes ABodyBuilder2 DTW structural similarity calculations\n",
      "\n",
      "ðŸŽ¯ Next Steps:\n",
      "  1. ðŸ“Š Open the Excel report for comprehensive performance comparison\n",
      "  2. ðŸ” Compare with train vs test results to understand differences\n",
      "  3. ðŸ“ˆ Analyze performance patterns in test vs test setting\n",
      "  4. ðŸ“‹ Share results with your research team\n",
      "  5. ðŸ“ Consider implications for real-world performance\n",
      "\n",
      "ðŸ’¡ Model Coverage Summary:\n",
      "  â€¢ Total unique models: 12\n",
      "  â€¢ Embedding-based models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2,\n",
      "    AbLang-Heavy, AntiBERTy, BALM, ESM-2, IgBERT, Parapred\n",
      "  â€¢ Sequence-based models: SEQID, CDRH3ID\n",
      "  â€¢ ABodyBuilder2 DTW: Excluded from this analysis\n",
      "  â€¢ Total configurations: 24\n",
      "\n",
      "ðŸ Test vs test benchmarking pipeline completed!\n",
      "\n",
      "ðŸ“„ Report: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "ðŸ“ Output folder: output_csvs_test\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST vs TEST PIPELINE COMPLETION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ”§ Configuration:\")\n",
    "print(f\"  â€¢ Analysis type: TEST vs TEST comparisons\")\n",
    "print(f\"  â€¢ Threshold optimization: VAL vs VAL\")\n",
    "print(f\"  â€¢ Recalculated embeddings: {RECALCULATE_EMBEDDINGS}\")\n",
    "print(f\"  â€¢ Used precomputed thresholds: {use_precomputed}\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Benchmarking Results:\")\n",
    "print(f\"  â€¢ Total configurations possible: {len(CONFIGS)}\")\n",
    "print(f\"  â€¢ Configurations attempted: {len(available_configs)}\")\n",
    "print(f\"  â€¢ Successful runs: {successful_configs}\")\n",
    "print(f\"  â€¢ Failed runs: {len(failed_configs)}\")\n",
    "\n",
    "if os.path.exists(os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)):\n",
    "    print(f\"\\nðŸ“Š Excel Report:\")\n",
    "    print(f\"  â€¢ Status: âœ… Generated successfully\")\n",
    "    print(f\"  â€¢ Location: {os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)}\")\n",
    "    print(f\"  â€¢ Ready for analysis and sharing\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Excel Report:\")\n",
    "    print(f\"  â€¢ Status: âŒ Generation failed\")\n",
    "    print(f\"  â€¢ Check error messages above\")\n",
    "\n",
    "print(f\"\\nðŸ”¬ Models Configured for Benchmarking:\")\n",
    "all_models = set()\n",
    "for config_name, config in CONFIGS.items():\n",
    "    all_models.add(f\"{config['model_name']} ({config['score_type']})\")\n",
    "        \n",
    "for model in sorted(all_models):\n",
    "    print(f\"  â€¢ {model}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Datasets Configured:\")\n",
    "datasets_configured = set()\n",
    "for config_name, config in CONFIGS.items():\n",
    "    datasets_configured.add(config['dataset_type'].upper())\n",
    "        \n",
    "for dataset in sorted(datasets_configured):\n",
    "    print(f\"  â€¢ {dataset}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Key Differences from Train vs Test Analysis:\")\n",
    "print(f\"  1. ðŸ”„ Compares TEST antibodies against other TEST antibodies\")\n",
    "print(f\"  2. ðŸŽ¯ Uses VAL vs VAL for F1 threshold optimization\")\n",
    "print(f\"  3. ðŸ“Š Uses test_label_mat.pt and val_label_mat.pt matrices\")\n",
    "print(f\"  4. ðŸš« Excludes ABodyBuilder2 DTW structural similarity calculations\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(f\"  1. ðŸ“Š Open the Excel report for comprehensive performance comparison\")\n",
    "print(f\"  2. ðŸ” Compare with train vs test results to understand differences\")\n",
    "print(f\"  3. ðŸ“ˆ Analyze performance patterns in test vs test setting\")\n",
    "print(f\"  4. ðŸ“‹ Share results with your research team\")\n",
    "print(f\"  5. ðŸ“ Consider implications for real-world performance\")\n",
    "\n",
    "if failed_configs:\n",
    "    print(f\"\\nâš ï¸ Failed Configurations to Investigate:\")\n",
    "    for config in failed_configs:\n",
    "        print(f\"  â€¢ {config}: {execution_results[config]}\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"\\nâ“ Configurations Not Attempted (Missing Files):\")\n",
    "    for config in missing_configs:\n",
    "        print(f\"  â€¢ {config}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Model Coverage Summary:\")\n",
    "print(f\"  â€¢ Total unique models: {len(all_models)}\")\n",
    "print(f\"  â€¢ Embedding-based models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2,\")\n",
    "print(f\"    AbLang-Heavy, AntiBERTy, BALM, ESM-2, IgBERT, Parapred\")\n",
    "print(f\"  â€¢ Sequence-based models: SEQID, CDRH3ID\")\n",
    "print(f\"  â€¢ ABodyBuilder2 DTW: Excluded from this analysis\")\n",
    "print(f\"  â€¢ Total configurations: {len(CONFIGS)}\")\n",
    "\n",
    "print(f\"\\nðŸ Test vs test benchmarking pipeline completed!\")\n",
    "print(f\"\\nðŸ“„ Report: {os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)}\")\n",
    "print(f\"ðŸ“ Output folder: {OUTPUT_FOLDER}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pub_clone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

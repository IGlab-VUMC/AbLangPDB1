{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test vs Test Benchmarking Pipeline with Excel Export\n",
    "\n",
    "This notebook provides a complete workflow for:\n",
    "1. Running comprehensive benchmarks across multiple models and datasets using **TEST vs TEST** comparisons\n",
    "2. Using **VAL vs VAL** comparisons for F1 threshold optimization\n",
    "3. Generating standardized summary files\n",
    "4. Creating formatted Excel reports with performance rankings\n",
    "\n",
    "**Models included:**\n",
    "- **AbLang Family**: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy (cosine similarity)\n",
    "- **Other Protein LMs**: AntiBERTy, BALM, ESM-2, IgBERT, Parapred (cosine similarity)\n",
    "- **Sequence-based**: SEQID (sequence identity), CDRH3ID (CDRH3 identity)\n",
    "- **Note**: ABodyBuilder2 DTW configurations are excluded from this analysis\n",
    "\n",
    "**Datasets:**\n",
    "- SAbDab (structural antibody database)\n",
    "- DMS (deep mutational scanning)\n",
    "\n",
    "**Key Differences from Train vs Test Analysis:**\n",
    "- Compares test antibodies against other test antibodies\n",
    "- Uses validation set for threshold optimization instead of training set\n",
    "- Uses test vs test label matrices and val vs val label matrices\n",
    "- Excludes ABodyBuilder2 structural similarity calculations\n",
    "\n",
    "**Features:**\n",
    "- Works with ALL available parquet files in the benchmarking directory\n",
    "- Optional embedding re-calculation toggle (disabled by default)\n",
    "- Optional threshold reuse for faster re-runs\n",
    "- Comprehensive error handling and logging\n",
    "- Supports 12 different models across both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  â€¢ Recalculate embeddings: False\n",
      "  â€¢ Recalculate summary metrics: False\n",
      "  â€¢ Output folder: output_csvs_test\n",
      "  â€¢ Excel filename: comprehensive_benchmarking_results_test.xlsx\n",
      "  â€¢ Batch size: 256\n",
      "\n",
      "ğŸ“ Note: This notebook supports test vs test comparisons:\n",
      "  â€¢ Models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy,\n",
      "           AntiBERTy, BALM, ESM-2, IgBERT, Parapred, SEQID, CDRH3ID\n",
      "  â€¢ Datasets: SAbDab and DMS\n",
      "  â€¢ Comparison type: TEST vs TEST (thresholded using VAL vs VAL)\n",
      "  â€¢ ABodyBuilder2 DTW configurations excluded\n",
      "  â€¢ Only existing parquet files will be processed\n",
      "  â€¢ Summary metrics will be reused if available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION FLAGS - MODIFY THESE AS NEEDED\n",
    "# =============================================================================\n",
    "\n",
    "# Set to False to use existing parquet files (faster), True to re-calculate embeddings\n",
    "# NOTE: This notebook now works with ALL available parquet files in the directory\n",
    "RECALCULATE_EMBEDDINGS = False\n",
    "\n",
    "# Set to False to skip benchmark recalculation if summary files exist (faster), True to always recalculate\n",
    "RECALCULATE_SUMMARYMETRICS = False\n",
    "\n",
    "# Model paths - update these paths as needed\n",
    "MODEL_PATHS = {\n",
    "    \"AbLangPDB\": \"../../../huggingface/AbLangPDB1/ablangpdb_model.safetensors\",\n",
    "    \"AbLangRBD\": \"../../../huggingface/AbLangRBD1/model.safetensors\"\n",
    "}\n",
    "\n",
    "# Batch size for embedding generation\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_FOLDER = \"output_csvs_test\"\n",
    "EXCEL_FILENAME = \"comprehensive_benchmarking_results_test.xlsx\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  â€¢ Recalculate embeddings: {RECALCULATE_EMBEDDINGS}\")\n",
    "print(f\"  â€¢ Recalculate summary metrics: {RECALCULATE_SUMMARYMETRICS}\")\n",
    "print(f\"  â€¢ Output folder: {OUTPUT_FOLDER}\")\n",
    "print(f\"  â€¢ Excel filename: {EXCEL_FILENAME}\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nğŸ“ Note: This notebook supports test vs test comparisons:\")\n",
    "print(f\"  â€¢ Models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy,\")\n",
    "print(f\"           AntiBERTy, BALM, ESM-2, IgBERT, Parapred, SEQID, CDRH3ID\")\n",
    "print(f\"  â€¢ Datasets: SAbDab and DMS\")\n",
    "print(f\"  â€¢ Comparison type: TEST vs TEST (thresholded using VAL vs VAL)\")\n",
    "print(f\"  â€¢ ABodyBuilder2 DTW configurations excluded\")\n",
    "print(f\"  â€¢ Only existing parquet files will be processed\")\n",
    "print(f\"  â€¢ Summary metrics will be {'recalculated' if RECALCULATE_SUMMARYMETRICS else 'reused if available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import typing as T\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import local modules\n",
    "import calculate_metrics\n",
    "import calculate_metrics_dms\n",
    "import models\n",
    "from excel_generator import generate_results_excel, print_summary_stats\n",
    "from ablangpaired_model import AbLangPairedConfig, AbLangPaired\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_file_exists(filepath, description=\"\"):\n",
    "    \"\"\"Check if a file exists and return status.\"\"\"\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = \"âœ…\" if exists else \"âŒ\"\n",
    "    print(f\"  {status} {description}: {filepath}\")\n",
    "    return exists\n",
    "\n",
    "\n",
    "def embed_with_ablangpaired(input_path: str, output_path: str, model_path: str, model_name: str):\n",
    "    \"\"\"Generate embeddings using AbLangPaired models.\n",
    "    \n",
    "        Args:\n",
    "            model_name: str. If \"ablangpre\" then the model architecture will no longer have the mixer layer.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ Generating {model_name} embeddings...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_parquet(input_path)\n",
    "    if \"EMBEDDING\" in df.columns:\n",
    "        df = df.drop(columns=[\"EMBEDDING\"])\n",
    "    \n",
    "    # Setup model\n",
    "    model_config = AbLangPairedConfig(checkpoint_filename=model_path)\n",
    "    is_ablangpre = model_name == \"ablangpre\"\n",
    "    model = AbLangPaired(model_config, device=device, use_pretrained=is_ablangpre)\n",
    "    \n",
    "    # Tokenize and embed using enhanced methods\n",
    "    tokenized_dataloader = models.tokenize_data(df, model_config, batch_size=BATCH_SIZE)\n",
    "    all_embeds = models.embed_dataloader(tokenized_dataloader, model, device)\n",
    "    \n",
    "    # Save\n",
    "    df['EMBEDDING'] = list(all_embeds.cpu().numpy())\n",
    "    df.to_parquet(output_path)\n",
    "    \n",
    "    print(f\"âœ… {model_name} embeddings saved to {output_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_summary_file_paths(model_name: str, dataset_type: str, output_folder: str):\n",
    "    \"\"\"Get expected summary file paths for a model/dataset combination.\"\"\"\n",
    "    if dataset_type == \"sabdab\":\n",
    "        # SAbDab has both epitope and antigen summary files\n",
    "        epitope_file = os.path.join(output_folder, f\"{model_name}_sabdab_ep_summarymetrics.txt\")\n",
    "        antigen_file = os.path.join(output_folder, f\"{model_name}_sabdab_ag_summarymetrics.txt\")\n",
    "        return [epitope_file, antigen_file]\n",
    "    elif dataset_type == \"dms\":\n",
    "        # DMS has only one summary file\n",
    "        dms_file = os.path.join(output_folder, f\"{model_name}_dms_summarymetrics.txt\")\n",
    "        return [dms_file]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def read_and_display_summary_results(summary_files: list, model_name: str, dataset_type: str):\n",
    "    \"\"\"Read and display results from existing summary files.\"\"\"\n",
    "    print(f\"\\nğŸ“Š Loading existing results for {model_name} on {dataset_type}:\")\n",
    "    \n",
    "    for summary_file in summary_files:\n",
    "        if os.path.exists(summary_file):\n",
    "            try:\n",
    "                with open(summary_file, 'r') as f:\n",
    "                    content = f.read().strip()\n",
    "                \n",
    "                # Extract the task type from filename\n",
    "                if \"sabdab_ep\" in summary_file:\n",
    "                    task_type = \"Epitope-level performance\"\n",
    "                elif \"sabdab_ag\" in summary_file:\n",
    "                    task_type = \"Antigen-level performance\"\n",
    "                else:\n",
    "                    task_type = \"Performance\"\n",
    "                \n",
    "                print(f\"\\n--- {task_type} ---\")\n",
    "                print(content)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error reading {summary_file}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Summary file not found: {summary_file}\")\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Required Base Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Checking base dataset files...\n",
      "  âœ… SAbDab base dataset: ablangpdb_renameddatasets.parquet\n",
      "  âœ… DMS base dataset: ablangrbd_renameddatasets.parquet\n",
      "  âœ… SAbDab validation labels (test vs test): ablangpdb_val_label_mat.pt\n",
      "  âœ… SAbDab test labels (test vs test): ablangpdb_test_label_mat.pt\n",
      "  âœ… DMS validation labels (test vs test): dms_val_label_mat.pt\n",
      "  âœ… DMS test labels (test vs test): dms_test_label_mat.pt\n",
      "\n",
      "âœ… All base files found!\n"
     ]
    }
   ],
   "source": [
    "# Check that base dataset files exist\n",
    "print(\"ğŸ“‹ Checking base dataset files...\")\n",
    "\n",
    "base_files = {\n",
    "    \"SAbDab base dataset\": \"ablangpdb_renameddatasets.parquet\",\n",
    "    \"DMS base dataset\": \"ablangrbd_renameddatasets.parquet\",\n",
    "    \"SAbDab validation labels (test vs test)\": \"ablangpdb_val_label_mat.pt\",\n",
    "    \"SAbDab test labels (test vs test)\": \"ablangpdb_test_label_mat.pt\",\n",
    "    \"DMS validation labels (test vs test)\": \"dms_val_label_mat.pt\",\n",
    "    \"DMS test labels (test vs test)\": \"dms_test_label_mat.pt\"\n",
    "}\n",
    "\n",
    "missing_base_files = []\n",
    "for desc, filepath in base_files.items():\n",
    "    if not check_file_exists(filepath, desc):\n",
    "        missing_base_files.append(filepath)\n",
    "\n",
    "if missing_base_files:\n",
    "    raise FileNotFoundError(f\"Missing required base files: {missing_base_files}\")\n",
    "\n",
    "print(\"\\nâœ… All base files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for consolidated embedding files\nconsolidated_files = {\n    \"sabdab_embeddedby_many.parquet\": \"SAbDab consolidated embeddings (all models)\",\n    \"dms_embeddedby_many.parquet\": \"DMS consolidated embeddings (all models)\"\n}\n\nprint(f\"\\n{'='*60}\")\nprint(\"CONSOLIDATED EMBEDDING FILE STATUS CHECK\")\nprint(f\"{'='*60}\")\n\nprint(\" Checking for consolidated embedding files...\")\n\nexisting_files = []\nmissing_files = []\n\nfor file_path, description in consolidated_files.items():\n    if os.path.exists(file_path):\n        print(f\"âœ… Found: {description} ({file_path})\")\n        existing_files.append(file_path)\n    else:\n        print(f\"âŒ Missing: {description} ({file_path})\")\n        missing_files.append(file_path)\n\nprint(f\"\\nğŸ“Š Consolidated Files Summary:\")\nprint(f\"  â€¢ Existing files: {len(existing_files)}/{len(consolidated_files)}\")\nprint(f\"  â€¢ Missing files: {len(missing_files)}\")\n\nif missing_files:\n    print(f\"\\nâš ï¸ Missing consolidated embedding files:\")\n    for file in missing_files:\n        print(f\"  - {file}\")\n    print(\"\\nNote: Configurations using missing files will be skipped automatically.\")\n\nprint(f\"\\nâœ… Consolidated embedding file check complete!\")\n\n# Display available embedding columns if files exist\nif existing_files:\n    print(f\"\\nğŸ“Š Available embedding columns:\")\n    for file_path in existing_files:\n        try:\n            df = pd.read_parquet(file_path)\n            embedding_cols = [col for col in df.columns if col.startswith('EMBEDDING_')]\n            print(f\"  â€¢ {file_path}: {len(embedding_cols)} models\")\n            for col in sorted(embedding_cols):\n                model_name = col.replace('EMBEDDING_', '')\n                print(f\"    - {model_name}\")\n        except Exception as e:\n            print(f\"  â€¢ {file_path}: Error reading file - {str(e)}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify All Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check that all required files now exist\nprint(\"\\nğŸ“‹ Verifying all required files...\")\n\nall_required_files = {\n    # Consolidated embedding datasets\n    \"SAbDab consolidated embeddings\": \"sabdab_embeddedby_many.parquet\",\n    \"DMS consolidated embeddings\": \"dms_embeddedby_many.parquet\",\n    \n    # Label matrices (test vs test)\n    \"SAbDab validation labels (test vs test)\": \"ablangpdb_val_label_mat.pt\",\n    \"SAbDab test labels (test vs test)\": \"ablangpdb_test_label_mat.pt\",\n    \"DMS validation labels (test vs test)\": \"dms_val_label_mat.pt\",\n    \"DMS test labels (test vs test)\": \"dms_test_label_mat.pt\"\n}\n\nmissing_files = []\nfor desc, filepath in all_required_files.items():\n    if not check_file_exists(filepath, desc):\n        missing_files.append(filepath)\n\nif missing_files:\n    print(f\"\\nâš ï¸ Warning: {len(missing_files)} files are missing:\")\n    for file in missing_files:\n        print(f\"  - {file}\")\n    print(\"\\nProceeding with available files only.\")\nelse:\n    print(\"\\nâœ… All required files are available!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete configuration for all model/dataset/metric combinations\nCONFIGS = {\n    # SAbDab Dataset Configurations - Use consolidated file\n    \"ablangpdb_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"AbLangPDB\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ABLANGPDB\"\n    },\n    \"ablangrbd_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"AbLangRBD\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ABLANGRBD\"\n    },\n    \"ablangpre_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"AbLangPre\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ABLANGPRE\"\n    },\n    \"ablang2_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"AbLang2\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ABLANG2\"\n    },\n    \"ablang_heavy_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"AbLang-Heavy\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ABLANG-HEAVY\"\n    },\n    \"antiberty_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"AntiBERTy\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ANTIBERTY\"\n    },\n    \"balm_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"BALM\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_BALM\"\n    },\n    \"esm2_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"ESM-2\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ESM-2\"\n    },\n    \"igbert_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"IgBERT\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_IGBERT\"\n    },\n    \"parapred_sabdab_cosine\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"Parapred\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_PARAPRED\"\n    },\n    \"seqid_sabdab\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"SEQID\",\n        \"score_type\": \"seq_identity\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ABLANGPDB\"  # Doesn't matter for seq_identity\n    },\n    \"cdrh3id_sabdab\": {\n        \"df_path\": \"sabdab_embeddedby_many.parquet\",\n        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n        \"model_name\": \"CDRH3ID\",\n        \"score_type\": \"cdrh3_identity\",\n        \"function\": calculate_metrics.get_metrics,\n        \"dataset_type\": \"sabdab\",\n        \"embedding_column\": \"EMBEDDING_ABLANGPDB\"  # Doesn't matter for cdrh3_identity\n    },\n    \n    # DMS Dataset Configurations - Use consolidated file\n    \"ablangpdb_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"AbLangPDB\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ABLANGPDB\"\n    },\n    \"ablangrbd_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"AbLangRBD\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ABLANGRBD\"\n    },\n    \"ablangpre_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"AbLangPre\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ABLANGPRE\"\n    },\n    \"ablang2_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"AbLang2\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ABLANG2\"\n    },\n    \"ablang_heavy_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"AbLang-Heavy\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ABLANG-HEAVY\"\n    },\n    \"antiberty_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"AntiBERTy\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ANTIBERTY\"\n    },\n    \"balm_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"BALM\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_BALM\"\n    },\n    \"esm2_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"ESM-2\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ESM-2\"\n    },\n    \"igbert_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"IgBERT\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_IGBERT\"\n    },\n    \"parapred_dms_cosine\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"Parapred\",\n        \"score_type\": \"cosine\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_PARAPRED\"\n    },\n    \"seqid_dms\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"SEQID\",\n        \"score_type\": \"seq_identity\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ABLANGRBD\"  # Doesn't matter for seq_identity\n    },\n    \"cdrh3id_dms\": {\n        \"df_path\": \"dms_embeddedby_many.parquet\",\n        \"labels_val\": \"dms_val_label_mat.pt\",\n        \"labels_test\": \"dms_test_label_mat.pt\",\n        \"model_name\": \"CDRH3ID\",\n        \"score_type\": \"cdrh3_identity\",\n        \"function\": calculate_metrics_dms.get_metrics_dms,\n        \"dataset_type\": \"dms\",\n        \"embedding_column\": \"EMBEDDING_ABLANGRBD\"  # Doesn't matter for cdrh3_identity\n    }\n}\n\nprint(f\"Configured {len(CONFIGS)} model/dataset/metric combinations:\")\nfor name, config in CONFIGS.items():\n    print(f\"  â€¢ {name}: {config['model_name']} on {config['dataset_type']} using {config['score_type']}\")\n    \nprint(f\"\\nğŸ“ Note: ABodyBuilder2 DTW configurations have been excluded from this analysis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Available Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ablangpdb_sabdab_cosine: Ready to run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ablangrbd_sabdab_cosine: Ready to run\n",
      "âœ… ablangpre_sabdab_cosine: Ready to run\n",
      "âœ… ablang2_sabdab_cosine: Ready to run\n",
      "âœ… ablang_heavy_sabdab_cosine: Ready to run\n",
      "âœ… antiberty_sabdab_cosine: Ready to run\n",
      "âœ… balm_sabdab_cosine: Ready to run\n",
      "âœ… esm2_sabdab_cosine: Ready to run\n",
      "âœ… igbert_sabdab_cosine: Ready to run\n",
      "âœ… parapred_sabdab_cosine: Ready to run\n",
      "âœ… seqid_sabdab: Ready to run\n",
      "âœ… cdrh3id_sabdab: Ready to run\n",
      "âœ… ablangpdb_dms_cosine: Ready to run\n",
      "âœ… ablangrbd_dms_cosine: Ready to run\n",
      "âœ… ablangpre_dms_cosine: Ready to run\n",
      "âœ… ablang2_dms_cosine: Ready to run\n",
      "âœ… ablang_heavy_dms_cosine: Ready to run\n",
      "âœ… antiberty_dms_cosine: Ready to run\n",
      "âœ… balm_dms_cosine: Ready to run\n",
      "âœ… esm2_dms_cosine: Ready to run\n",
      "âœ… igbert_dms_cosine: Ready to run\n",
      "âœ… parapred_dms_cosine: Ready to run\n",
      "âœ… seqid_dms: Ready to run\n",
      "âœ… cdrh3id_dms: Ready to run\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "  â€¢ Available configurations: 24/24\n",
      "  â€¢ Missing configurations: 0\n"
     ]
    }
   ],
   "source": [
    "# Check which configurations can actually run based on available files\n",
    "available_configs = {}\n",
    "missing_configs = []\n",
    "\n",
    "for config_name, config in CONFIGS.items():\n",
    "    files_to_check = [config[\"df_path\"], config[\"labels_val\"], config[\"labels_test\"]]\n",
    "    missing_files = [f for f in files_to_check if not os.path.exists(f)]\n",
    "    \n",
    "    if not missing_files:\n",
    "        available_configs[config_name] = config\n",
    "        print(f\"âœ… {config_name}: Ready to run\")\n",
    "    else:\n",
    "        missing_configs.append(config_name)\n",
    "        print(f\"âŒ {config_name}: Missing files - {missing_files}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "print(f\"  â€¢ Available configurations: {len(available_configs)}/{len(CONFIGS)}\")\n",
    "print(f\"  â€¢ Missing configurations: {len(missing_configs)}\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"\\nâš ï¸ Configurations that will be skipped: {', '.join(missing_configs)}\")\n",
    "\n",
    "if not available_configs:\n",
    "    raise RuntimeError(\"âŒ No configurations are available to run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Pre-computed Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ†• Computing fresh thresholds for all configurations\n",
      "ğŸ“ Note: Test vs test comparisons require new threshold optimization\n"
     ]
    }
   ],
   "source": [
    "# Optional: Pre-computed thresholds to skip threshold optimization\n",
    "# NOTE: Thresholds from train vs test analysis are not valid for test vs test comparisons\n",
    "# These thresholds need to be recalculated for the new comparison type\n",
    "\n",
    "PRECOMPUTED_THRESHOLDS = {\n",
    "    # Thresholds will need to be recalculated for test vs test comparisons\n",
    "    # The original thresholds were optimized for train vs test, not test vs test\n",
    "    # Leave empty to force recalculation\n",
    "}\n",
    "\n",
    "use_precomputed = len(PRECOMPUTED_THRESHOLDS) > 0\n",
    "if use_precomputed:\n",
    "    print(f\"ğŸ”„ Using precomputed thresholds for {len(PRECOMPUTED_THRESHOLDS)} configurations\")\n",
    "    for config_name, thresholds in PRECOMPUTED_THRESHOLDS.items():\n",
    "        print(f\"  â€¢ {config_name}: {thresholds}\")\n",
    "else:\n",
    "    print(\"ğŸ†• Computing fresh thresholds for all configurations\")\n",
    "    print(\"ğŸ“ Note: Test vs test comparisons require new threshold optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "DATASET",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ef0161e9-02df-4b90-ada4-c6cf6ee645df",
       "rows": [
        [
         "TRAIN",
         "1517"
        ],
        [
         "TEST",
         "202"
        ],
        [
         "VAL",
         "190"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "DATASET\n",
       "TRAIN    1517\n",
       "TEST      202\n",
       "VAL       190\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('ablangpdb_renameddatasets.parquet')\n",
    "df[\"DATASET\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comprehensive Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute all available configurations\nexecution_results = {}\nfailed_configs = []\n\nprint(f\"\\n{'='*70}\")\nprint(f\"COMPREHENSIVE BENCHMARK EXECUTION\")\nprint(f\"{'='*70}\")\nprint(f\"Total configurations to run: {len(available_configs)}\")\nprint(f\"Recalculate summary metrics: {RECALCULATE_SUMMARYMETRICS}\")\n\nfor i, (config_name, config) in enumerate(available_configs.items(), 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"[{i}/{len(available_configs)}] Running: {config_name}\")\n    print(f\"Model: {config['model_name']}, Dataset: {config['dataset_type']}, Score: {config['score_type']}\")\n    print(f\"{'='*70}\")\n    \n    # Check if summary files already exist and flag is False\n    if not RECALCULATE_SUMMARYMETRICS:\n        summary_files = get_summary_file_paths(config['model_name'], config['dataset_type'], OUTPUT_FOLDER)\n        all_summaries_exist = all(os.path.exists(f) for f in summary_files)\n        \n        if all_summaries_exist:\n            print(f\"ğŸ“‹ Summary files already exist, skipping recalculation...\")\n            read_and_display_summary_results(summary_files, config['model_name'], config['dataset_type'])\n            execution_results[config_name] = \"âœ… Loaded from existing files\"\n            print(f\"\\nâœ… [{i}/{len(available_configs)}] {config_name} loaded from existing files!\")\n            continue\n        else:\n            print(f\"ğŸ“‹ Some summary files missing, proceeding with calculation...\")\n    \n    try:\n        # Prepare arguments for TEST vs TEST comparisons\n        args = {\n            \"df_path\": config[\"df_path\"],\n            \"labels_file_val\": config[\"labels_val\"],\n            \"labels_file_test\": config[\"labels_test\"],\n            \"score_type\": config[\"score_type\"],\n            \"model_name\": config[\"model_name\"],\n            \"output_folder\": OUTPUT_FOLDER,\n            # CRITICAL FIX: Use correct dataset parameters for test vs test\n            \"dataset1\": \"VAL\",           # Use VAL for threshold optimization (VAL vs VAL)\n            \"dataset2_val\": \"VAL\",       # VAL vs VAL for F1 threshold finding\n            \"dataset2_test\": \"TEST\",     # TEST vs TEST for final evaluation\n            \"use_square_matrices\": True  # Enable proper square matrix handling\n        }\n        \n        # Add embedding_column if it exists in config\n        if \"embedding_column\" in config:\n            args[\"embedding_column\"] = config[\"embedding_column\"]\n        \n        # Add matrix file paths for ABodyBuilder2 DTW configurations (if they exist)\n        if config[\"score_type\"] == \"abodybuilder2_dtw_cdrs\":\n            args[\"matrix_file_val\"] = config[\"matrix_file_val\"]\n            args[\"matrix_file_test\"] = config[\"matrix_file_test\"]\n        \n        # Add precomputed thresholds if available\n        if config_name in PRECOMPUTED_THRESHOLDS:\n            thresholds = PRECOMPUTED_THRESHOLDS[config_name]\n            if config[\"dataset_type\"] == \"sabdab\":\n                if \"epitope_threshold\" in thresholds:\n                    args[\"epitope_threshold\"] = thresholds[\"epitope_threshold\"]\n                if \"antigen_threshold\" in thresholds:\n                    args[\"antigen_threshold\"] = thresholds[\"antigen_threshold\"]\n            elif config[\"dataset_type\"] == \"dms\":\n                if \"epitope_threshold\" in thresholds:\n                    args[\"epitope_threshold\"] = thresholds[\"epitope_threshold\"]\n        \n        # Execute the benchmark\n        print(f\"ğŸ”„ Running with parameters:\")\n        print(f\"  â€¢ Threshold optimization: VAL vs VAL\")\n        print(f\"  â€¢ Final evaluation: TEST vs TEST\")\n        print(f\"  â€¢ Square matrix mode: ENABLED\")\n        if \"embedding_column\" in config:\n            print(f\"  â€¢ Embedding column: {config['embedding_column']}\")\n        \n        config[\"function\"](**args)\n        \n        execution_results[config_name] = \"âœ… Success\"\n        print(f\"\\nâœ… [{i}/{len(available_configs)}] {config_name} completed successfully!\")\n        \n    except Exception as e:\n        error_msg = f\"âŒ Error: {str(e)}\"\n        execution_results[config_name] = error_msg\n        failed_configs.append(config_name)\n        print(f\"\\nâŒ [{i}/{len(available_configs)}] {config_name} failed: {str(e)}\")\n        continue\n\nprint(f\"\\n{'='*70}\")\nprint(\"COMPREHENSIVE BENCHMARK EXECUTION SUMMARY\")\nprint(f\"{'='*70}\")\n\nfor config_name, result in execution_results.items():\n    print(f\"{config_name:30} {result}\")\n\nsuccessful_configs = len(available_configs) - len(failed_configs)\nprint(f\"\\nğŸ“Š Results:\")\nprint(f\"  â€¢ Successful: {successful_configs}/{len(available_configs)}\")\nprint(f\"  â€¢ Failed: {len(failed_configs)}\")\n\nif failed_configs:\n    print(f\"\\nâš ï¸ Failed configurations: {', '.join(failed_configs)}\")\nelse:\n    print(\"\\nğŸ‰ All available configurations completed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Excel Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Generating summary statistics...\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Total summary files found: 36\n",
      "Unique models: 12 (AbLang-Heavy, AbLang2, AbLangPDB, AbLangPre, AbLangRBD, AntiBERTy, BALM, CDRH3ID, ESM-2, IgBERT, Parapred, SEQID)\n",
      "Unique datasets: 3 (dms, sabdab_ag, sabdab_ep)\n",
      "Unique score types: 3 (cdrh3_identity, cosine, seq_identity)\n",
      "Best ROC_AUC: AbLangRBD on dms (0.7279)\n",
      "Best Average_Precision: AbLangRBD on dms (0.3944)\n",
      "Best F1_Score: AbLangRBD on dms (0.3859)\n",
      "\n",
      "======================================================================\n",
      "GENERATING COMPREHENSIVE EXCEL REPORT\n",
      "======================================================================\n",
      "Collecting summary metrics from output_csvs_test...\n",
      "Found 36 summary files\n",
      "Creating pivot table...\n",
      "Pivot table created with 12 models and 9 metric columns\n",
      "Ranking values for formatting...\n",
      "Exporting to Excel: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "âœ… Excel file generated successfully: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "\n",
      "ğŸ‰ Comprehensive Excel report generated successfully!\n",
      "ğŸ“ File location: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "ğŸ“ File size: 5,887 bytes\n",
      "\n",
      "ğŸ“– Excel Report Contents:\n",
      "  â€¢ Models as rows (AbLangPDB, AbLangRBD, AbLangPre, SEQID, CDRH3ID)\n",
      "  â€¢ Datasets grouped as column headers (SAbDab, DMS)\n",
      "  â€¢ Metrics: ROC-AUC, Average Precision, F1 Score\n",
      "  â€¢ Best performance: Bold formatting\n",
      "  â€¢ Second best: Italic formatting\n",
      "  â€¢ Values rounded to 4 decimal places\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics\n",
    "print(\"\\nğŸ“Š Generating summary statistics...\")\n",
    "print_summary_stats(OUTPUT_FOLDER)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING COMPREHENSIVE EXCEL REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Generate the Excel file\n",
    "    excel_path = generate_results_excel(\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        excel_filename=EXCEL_FILENAME\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Comprehensive Excel report generated successfully!\")\n",
    "    print(f\"ğŸ“ File location: {excel_path}\")\n",
    "    print(f\"ğŸ“ File size: {os.path.getsize(excel_path):,} bytes\")\n",
    "    \n",
    "    # Provide usage instructions\n",
    "    print(f\"\\nğŸ“– Excel Report Contents:\")\n",
    "    print(f\"  â€¢ Models as rows (AbLangPDB, AbLangRBD, AbLangPre, SEQID, CDRH3ID)\")\n",
    "    print(f\"  â€¢ Datasets grouped as column headers (SAbDab, DMS)\")\n",
    "    print(f\"  â€¢ Metrics: ROC-AUC, Average Precision, F1 Score\")\n",
    "    print(f\"  â€¢ Best performance: Bold formatting\")\n",
    "    print(f\"  â€¢ Second best: Italic formatting\")\n",
    "    print(f\"  â€¢ Values rounded to 4 decimal places\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error generating Excel report: {str(e)}\")\n",
    "    print(\"\\nDebugging information:\")\n",
    "    print(f\"  â€¢ Output folder: {OUTPUT_FOLDER}\")\n",
    "    print(f\"  â€¢ Files in folder: {len(os.listdir(OUTPUT_FOLDER))}\")\n",
    "    \n",
    "    # List summary files found\n",
    "    import glob\n",
    "    summary_files = glob.glob(os.path.join(OUTPUT_FOLDER, \"*summarymetrics.txt\"))\n",
    "    print(f\"  â€¢ Summary files found: {len(summary_files)}\")\n",
    "    for f in summary_files[:5]:  # Show first 5\n",
    "        print(f\"    - {os.path.basename(f)}\")\n",
    "    if len(summary_files) > 5:\n",
    "        print(f\"    - ... and {len(summary_files)-5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST vs TEST PIPELINE COMPLETION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ”§ Configuration:\n",
      "  â€¢ Analysis type: TEST vs TEST comparisons\n",
      "  â€¢ Threshold optimization: VAL vs VAL\n",
      "  â€¢ Recalculated embeddings: False\n",
      "  â€¢ Used precomputed thresholds: False\n",
      "  â€¢ Batch size: 256\n",
      "\n",
      "ğŸ“ˆ Benchmarking Results:\n",
      "  â€¢ Total configurations possible: 24\n",
      "  â€¢ Configurations attempted: 24\n",
      "  â€¢ Successful runs: 24\n",
      "  â€¢ Failed runs: 0\n",
      "\n",
      "ğŸ“Š Excel Report:\n",
      "  â€¢ Status: âœ… Generated successfully\n",
      "  â€¢ Location: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "  â€¢ Ready for analysis and sharing\n",
      "\n",
      "ğŸ”¬ Models Configured for Benchmarking:\n",
      "  â€¢ AbLang-Heavy (cosine)\n",
      "  â€¢ AbLang2 (cosine)\n",
      "  â€¢ AbLangPDB (cosine)\n",
      "  â€¢ AbLangPre (cosine)\n",
      "  â€¢ AbLangRBD (cosine)\n",
      "  â€¢ AntiBERTy (cosine)\n",
      "  â€¢ BALM (cosine)\n",
      "  â€¢ CDRH3ID (cdrh3_identity)\n",
      "  â€¢ ESM-2 (cosine)\n",
      "  â€¢ IgBERT (cosine)\n",
      "  â€¢ Parapred (cosine)\n",
      "  â€¢ SEQID (seq_identity)\n",
      "\n",
      "ğŸ“Š Datasets Configured:\n",
      "  â€¢ DMS\n",
      "  â€¢ SABDAB\n",
      "\n",
      "ğŸ¯ Key Differences from Train vs Test Analysis:\n",
      "  1. ğŸ”„ Compares TEST antibodies against other TEST antibodies\n",
      "  2. ğŸ¯ Uses VAL vs VAL for F1 threshold optimization\n",
      "  3. ğŸ“Š Uses test_label_mat.pt and val_label_mat.pt matrices\n",
      "  4. ğŸš« Excludes ABodyBuilder2 DTW structural similarity calculations\n",
      "\n",
      "ğŸ¯ Next Steps:\n",
      "  1. ğŸ“Š Open the Excel report for comprehensive performance comparison\n",
      "  2. ğŸ” Compare with train vs test results to understand differences\n",
      "  3. ğŸ“ˆ Analyze performance patterns in test vs test setting\n",
      "  4. ğŸ“‹ Share results with your research team\n",
      "  5. ğŸ“ Consider implications for real-world performance\n",
      "\n",
      "ğŸ’¡ Model Coverage Summary:\n",
      "  â€¢ Total unique models: 12\n",
      "  â€¢ Embedding-based models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2,\n",
      "    AbLang-Heavy, AntiBERTy, BALM, ESM-2, IgBERT, Parapred\n",
      "  â€¢ Sequence-based models: SEQID, CDRH3ID\n",
      "  â€¢ ABodyBuilder2 DTW: Excluded from this analysis\n",
      "  â€¢ Total configurations: 24\n",
      "\n",
      "ğŸ Test vs test benchmarking pipeline completed!\n",
      "\n",
      "ğŸ“„ Report: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "ğŸ“ Output folder: output_csvs_test\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST vs TEST PIPELINE COMPLETION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ”§ Configuration:\")\n",
    "print(f\"  â€¢ Analysis type: TEST vs TEST comparisons\")\n",
    "print(f\"  â€¢ Threshold optimization: VAL vs VAL\")\n",
    "print(f\"  â€¢ Recalculated embeddings: {RECALCULATE_EMBEDDINGS}\")\n",
    "print(f\"  â€¢ Used precomputed thresholds: {use_precomputed}\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Benchmarking Results:\")\n",
    "print(f\"  â€¢ Total configurations possible: {len(CONFIGS)}\")\n",
    "print(f\"  â€¢ Configurations attempted: {len(available_configs)}\")\n",
    "print(f\"  â€¢ Successful runs: {successful_configs}\")\n",
    "print(f\"  â€¢ Failed runs: {len(failed_configs)}\")\n",
    "\n",
    "if os.path.exists(os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)):\n",
    "    print(f\"\\nğŸ“Š Excel Report:\")\n",
    "    print(f\"  â€¢ Status: âœ… Generated successfully\")\n",
    "    print(f\"  â€¢ Location: {os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)}\")\n",
    "    print(f\"  â€¢ Ready for analysis and sharing\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š Excel Report:\")\n",
    "    print(f\"  â€¢ Status: âŒ Generation failed\")\n",
    "    print(f\"  â€¢ Check error messages above\")\n",
    "\n",
    "print(f\"\\nğŸ”¬ Models Configured for Benchmarking:\")\n",
    "all_models = set()\n",
    "for config_name, config in CONFIGS.items():\n",
    "    all_models.add(f\"{config['model_name']} ({config['score_type']})\")\n",
    "        \n",
    "for model in sorted(all_models):\n",
    "    print(f\"  â€¢ {model}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Datasets Configured:\")\n",
    "datasets_configured = set()\n",
    "for config_name, config in CONFIGS.items():\n",
    "    datasets_configured.add(config['dataset_type'].upper())\n",
    "        \n",
    "for dataset in sorted(datasets_configured):\n",
    "    print(f\"  â€¢ {dataset}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Key Differences from Train vs Test Analysis:\")\n",
    "print(f\"  1. ğŸ”„ Compares TEST antibodies against other TEST antibodies\")\n",
    "print(f\"  2. ğŸ¯ Uses VAL vs VAL for F1 threshold optimization\")\n",
    "print(f\"  3. ğŸ“Š Uses test_label_mat.pt and val_label_mat.pt matrices\")\n",
    "print(f\"  4. ğŸš« Excludes ABodyBuilder2 DTW structural similarity calculations\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Next Steps:\")\n",
    "print(f\"  1. ğŸ“Š Open the Excel report for comprehensive performance comparison\")\n",
    "print(f\"  2. ğŸ” Compare with train vs test results to understand differences\")\n",
    "print(f\"  3. ğŸ“ˆ Analyze performance patterns in test vs test setting\")\n",
    "print(f\"  4. ğŸ“‹ Share results with your research team\")\n",
    "print(f\"  5. ğŸ“ Consider implications for real-world performance\")\n",
    "\n",
    "if failed_configs:\n",
    "    print(f\"\\nâš ï¸ Failed Configurations to Investigate:\")\n",
    "    for config in failed_configs:\n",
    "        print(f\"  â€¢ {config}: {execution_results[config]}\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"\\nâ“ Configurations Not Attempted (Missing Files):\")\n",
    "    for config in missing_configs:\n",
    "        print(f\"  â€¢ {config}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Model Coverage Summary:\")\n",
    "print(f\"  â€¢ Total unique models: {len(all_models)}\")\n",
    "print(f\"  â€¢ Embedding-based models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2,\")\n",
    "print(f\"    AbLang-Heavy, AntiBERTy, BALM, ESM-2, IgBERT, Parapred\")\n",
    "print(f\"  â€¢ Sequence-based models: SEQID, CDRH3ID\")\n",
    "print(f\"  â€¢ ABodyBuilder2 DTW: Excluded from this analysis\")\n",
    "print(f\"  â€¢ Total configurations: {len(CONFIGS)}\")\n",
    "\n",
    "print(f\"\\nğŸ Test vs test benchmarking pipeline completed!\")\n",
    "print(f\"\\nğŸ“„ Report: {os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)}\")\n",
    "print(f\"ğŸ“ Output folder: {OUTPUT_FOLDER}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pub_clone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}